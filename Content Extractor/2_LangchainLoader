
### STEP 1 : Set up your environment 

import os
import time
import torch
import tensorflow
import flax
import pandas as pd 

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import AnyscaleEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone, ServerlessSpec
from langchain_community.vectorstores import Pinecone as PineconeStore 


# Set API keys | Embedding Model & Vector Storage 
os.environ["ANYSCALE_API_KEY"] = ""
os.environ["PINECONE_API_KEY"] = ""

os.environ["TOKENIZERS_PARALLELISM"] = "false"

### STEP 2 : Store knowledge in Pinecone 

## 2.1 Load your data [pdf]
#pdf_loader = PyMuPDFLoader('Sample_Blood.pdf')
pdf_loader = PyMuPDFLoader('')
Blood_reports = pdf_loader.load()

## 2.2 Text splitting [Based on chunks]
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
pdf_doc = text_splitter.split_documents(Blood_reports)

## 2.3 Initalize a LangChain embedding object [Anyscale]
api_key = os.getenv("ANYSCALE_API_KEY")
embeddings = AnyscaleEmbeddings(
    anyscale_api_key=api_key, model="thenlper/gte-large"
)

## 2.4 Create a serverless index in Pinecone for embedding storage 
# [Dims: 1024 & Metric: Cosine to match Anyscale]

pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))

index_name = "llms2"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name="lmms1",
        dimension=1024, 
        metric="cosine", 
        spec=ServerlessSpec(
            cloud="aws", 
            region="us-east-1"
        ) 
    ) 

## 2.5 Embed each chunk and upsert the embeddings into a distinct namespace
# Namespaces let you partition records within an index

namespace = "LMMS2"

docsearch = PineconeStore.from_documents(
    documents=pdf_doc,
    index_name=index_name,
    embedding=embeddings, 
    namespace=namespace 
)

time.sleep(1)

## 2.6 Use Pineconeâ€™s list and query operations to look at one of the records

index = pc.Index(index_name)

for ids in index.list(namespace=namespace):
    query = index.query(
        id=ids[0], 
        namespace=namespace, 
        top_k=1,
        include_values=True,
        include_metadata=True
    )
    print(query)

### Step 3 : Use the Chatbot 

## 3.1 Initialize a LangChain object 
# This object is for chatting with the gpt-3.5-turbo LLM and for including relevant context from Pinecone


## 3.2 Define a few questions about the specific Namespace  
# These questions require specific, private knowledge of the product, which the LLM does not have by default.

## 3.3 Send query1 to the LLM twice
# First with relevant knowledge from Pincone and then without any additional knowledge

## 3.4 Now repeat the process with query2


# Check: The first response should provide very accurate response, matching closely the information in the WonderVector5000 document, while the second response sounds convincing but is generic and inaccurate.


