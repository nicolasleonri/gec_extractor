
'''This code sets up a retrieval-augmented generation system using Anyscale's LLM and Pinecone for document retrieval, 
making it suitable for querying and generating responses based on the uploaded dataset.
'''

### STEP 1 : Set up your environment 

import os
import time
import torch
import tensorflow
import flax
import pandas as pd 

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import AnyscaleEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone, ServerlessSpec
from langchain_community.vectorstores import Pinecone as PineconeStore 
from langchain_community.chat_models import ChatAnyscale
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# Set API keys | Embedding Model & Vector Storage 
os.environ["ANYSCALE_API_KEY"] = ""
os.environ["PINECONE_API_KEY"] = ""
os.environ["TOKENIZERS_PARALLELISM"] = "false"

### STEP 2 : Store knowledge in Pinecone 

## 2.1 Load your data [pdf]
pdf_loader = PyMuPDFLoader('Sample_Blood.pdf')
Blood_reports = pdf_loader.load()

## 2.2 Text splitting [Based on chunks]
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
pdf_doc = text_splitter.split_documents(Blood_reports)

## 2.3 Initalize a LangChain embedding object [Anyscale]
api_key = os.getenv("ANYSCALE_API_KEY")
embeddings = AnyscaleEmbeddings(
    anyscale_api_key=api_key, model="thenlper/gte-large"
)

## 2.4 Create a serverless index in Pinecone for embedding storage 
# [Dims: 1024 & Metric: Cosine to match Anyscale]

pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))

index_name = "PP"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name="PP",
        dimension=1024, 
        metric="cosine", 
        spec=ServerlessSpec(
            cloud="aws", 
            region="us-east-1"
        ) 
    ) 

## 2.5 Embed each chunk and upsert the embeddings into a distinct namespace
# Namespaces let you partition records within an index

namespace = "PP1"
pineconeVector = PineconeStore.from_documents(
    documents=pdf_doc,
    index_name=index_name,
    embedding=embeddings, 
    namespace=namespace 
)

time.sleep(1)


### Step 3 : Use the Chatbot 

## 3.1 Initialize the language model with Anyscale
llm = ChatAnyscale(anyscale_api_key=os.getenv('ANYSCALE_API_KEY'), model = 'thenlper/gte-large')

# Initialize the Pinecone retriever
retriever = pineconeVector.as_retriever()
docs = retriever.invoke("what did he say about ketanji brown jackson")

# Define the system prompt | system prompt to instruct the language model on how to use the context to answer the question.
system_prompt = (
    "Use the given context to answer the question. "
    "If you don't know the answer, say you don't know. "
    "Use three sentences maximum and keep the answer concise. "
    "Context: {context}"
)

# Create the prompt template | prompt template that combines the system instructions and the human input.
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)
# Create the question-answer chain | creates a chain that combines documents (retrieved context) and generates an answer using the language model
question_answer_chain = create_stuff_documents_chain(llm, prompt)

# Create the retrieval chain | chain integrates the retriever (for fetching relevant documents) and the question-answer chain
chain = create_retrieval_chain(retriever, question_answer_chain)

# Query1 input
query =  "What is the hemoglobin count of Yash?"
#query2 = "What type of information is provided in this document?"

# Invoke the chain with the query
response = chain.invoke({"input": query})

# Print the response
print(response)


