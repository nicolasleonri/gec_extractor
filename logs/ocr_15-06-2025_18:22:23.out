2025-06-15 18:22:35.898555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-15 18:22:35.898689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-15 18:22:35.900575: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-15 18:22:35.908688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-15 18:22:36.758728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Starting processing of 54 images with 5 methods
Total tasks: 270
Looking for /home/azimuth/.keras-ocr/craft_mlt_25k.h5
Error: NEW ERROR: (InvalidArgument) Failed to parse program_desc from binary string.
  [Hint: Expected desc_.ParseFromString(binary_str) == true, but received desc_.ParseFromString(binary_str):0 != true:1.] (at /paddle/paddle/fluid/framework/program_desc.cc:152)

2025-06-15 18:22:40.522806: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.526348: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.527941: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.533236: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.534793: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.536321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.730452: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.731691: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.732798: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2025-06-15 18:22:40.733793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79181 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:05:00.0, compute capability: 8.0
Looking for /home/azimuth/.keras-ocr/craft_mlt_25k.h5
[2025-06-15 18:22:43,761] [ WARNING] deprecation.py:50 - From /data/gec_extractor/venv/ocr/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.
[2025-06-15 18:22:45,247] [   DEBUG] lstm.py:589 - Layer lstm_10 will use cuDNN kernels when running on GPU.
2025-06-15 18:22:45.300785: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
[2025-06-15 18:22:45,686] [   DEBUG] lstm.py:589 - Layer lstm_10_back will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:45,980] [   DEBUG] lstm.py:589 - Layer lstm_11 will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:46,303] [   DEBUG] lstm.py:589 - Layer lstm_11_back will use cuDNN kernels when running on GPU.
Looking for /home/azimuth/.keras-ocr/crnn_kurapan.h5
[2025-06-15 18:22:47,659] [   DEBUG] lstm.py:589 - Layer lstm will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:47,664] [   DEBUG] lstm.py:589 - Layer lstm will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:47,667] [   DEBUG] lstm.py:589 - Layer lstm will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:47,669] [   DEBUG] lstm.py:589 - Layer lstm_1 will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:47,678] [   DEBUG] lstm.py:589 - Layer lstm_1 will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:47,680] [   DEBUG] lstm.py:589 - Layer lstm_1 will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:48,442] [   DEBUG] lstm.py:589 - Layer lstm_10 will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:48,896] [   DEBUG] lstm.py:589 - Layer lstm_10_back will use cuDNN kernels when running on GPU.
[2025-06-15 18:22:49,557] [   DEBUG] lstm.py:589 - Layer lstm_11 will use cuDNN kernels when running on GPU.
2025-06-15 18:22:49.558580: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902
2025-06-15 18:22:49.714246: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
[2025-06-15 18:22:49,814] [   DEBUG] lstm.py:589 - Layer lstm_11_back will use cuDNN kernels when running on GPU.
Error: COMMON ERROR: CUDA out of memory. Tried to allocate 1.11 GiB. GPU 
Looking for /home/azimuth/.keras-ocr/crnn_kurapan.h5
Progress: 5/270 tasks completed
Error: COMMON ERROR: CUDA out of memory. Tried to allocate 1.23 GiB. GPU 
Error: NEW ERROR: (InvalidArgument) Failed to parse program_desc from binary string.
  [Hint: Expected desc_.ParseFromString(binary_str) == true, but received desc_.ParseFromString(binary_str):0 != true:1.] (at /paddle/paddle/fluid/framework/program_desc.cc:152)

[2025-06-15 18:23:05,231] [   DEBUG] lstm.py:589 - Layer lstm will use cuDNN kernels when running on GPU.
[2025-06-15 18:23:05,236] [   DEBUG] lstm.py:589 - Layer lstm will use cuDNN kernels when running on GPU.
[2025-06-15 18:23:05,240] [   DEBUG] lstm.py:589 - Layer lstm will use cuDNN kernels when running on GPU.
[2025-06-15 18:23:05,243] [   DEBUG] lstm.py:589 - Layer lstm_1 will use cuDNN kernels when running on GPU.
[2025-06-15 18:23:05,246] [   DEBUG] lstm.py:589 - Layer lstm_1 will use cuDNN kernels when running on GPU.
[2025-06-15 18:23:05,250] [   DEBUG] lstm.py:589 - Layer lstm_1 will use cuDNN kernels when running on GPU.
1/1 [==============================] - ETA: 0s1/1 [==============================] - 21s 21s/step
1/1 [==============================] - ETA: 0s1/1 [==============================] - 20s 20s/step
2025-06-15 18:23:16.454904: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:30] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_OUT_OF_MEMORY'

2025-06-15 18:23:16.455009: W tensorflow/core/framework/op_kernel.cc:1827] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'
2025-06-15 18:23:16.455135: W tensorflow/core/framework/op_kernel.cc:1827] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_UNKNOWN'
Error: NEW ERROR: Exception encountered when calling layer 'db_net' (type DBNet).

{{function_node __wrapped__Sigmoid_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Sigmoid] name: 

Call arguments received by layer 'db_net' (type DBNet):
  • x=tf.Tensor(shape=(1, 1024, 1024, 3), dtype=float32)
  • target=None
  • return_model_output=True
  • return_preds=True
  • kwargs={'training': 'False'}Looking for /home/azimuth/.keras-ocr/craft_mlt_25k.h5
Error: NEW ERROR: Exception encountered when calling layer 'db_net' (type DBNet).

{{function_node __wrapped__Sigmoid_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_UNKNOWN' [Op:Sigmoid] name: 

Call arguments received by layer 'db_net' (type DBNet):
  • x=tf.Tensor(shape=(1, 1024, 1024, 3), dtype=float32)
  • target=None
  • return_model_output=True
  • return_preds=True
  • kwargs={'training': 'False'}



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   tensorflow::py_dispatch::PythonAPIDispatcher::Dispatch(_object*, _object*)
1   tensorflow::py_dispatch::PyUnionChecker::Check(_object*)
2   tensorflow::py_dispatch::PyInstanceChecker::Check(_object*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1750011796 (unix time) try "date -d @1750011796" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x15b) received by PID 3042234 (TID 0x7fe5650fc640) from PID 347 ***]

