
Loading Image list...
Starting multithreaded LLM postprocessing...
Models to process: ['olmocr', 'rolmocr', 'nanonets']

============================================================
Starting **multiprocessing** for model: olmocr
Total items to process: 180
Using 4 worker processes
============================================================
olmocr Progress:   0%|          | 0/180 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[olmocr] Initializing VLMProcessor
Loading allenai/olmOCR-7B-0225-preview
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  9.65it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 26.23it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[olmocr] Initializing VLMProcessor
Loading allenai/olmOCR-7B-0225-preview
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  5.95it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 16.29it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[olmocr] Initializing VLMProcessor
Loading allenai/olmOCR-7B-0225-preview
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][olmocr] Initializing VLMProcessor
Loading allenai/olmOCR-7B-0225-preview
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  9.37it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  9.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 25.76it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 25.63it/s]
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   1%|          | 1/180 [00:16<49:18, 16.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   1%|          | 2/180 [01:51<3:06:47, 62.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   2%|▏         | 3/180 [02:03<1:56:25, 39.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   2%|▏         | 4/180 [02:06<1:13:43, 25.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
olmocr Progress:   3%|▎         | 5/180 [43:34<44:24:01, 913.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
olmocr Progress:   3%|▎         | 6/180 [45:39<31:10:51, 645.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   4%|▍         | 7/180 [45:48<21:00:49, 437.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   4%|▍         | 8/180 [45:54<14:19:40, 299.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   5%|▌         | 9/180 [45:58<9:51:01, 207.38s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   6%|▌         | 10/180 [46:28<7:12:15, 152.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   6%|▌         | 11/180 [46:32<5:02:12, 107.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   7%|▋         | 12/180 [46:46<3:40:31, 78.76s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   7%|▋         | 13/180 [47:18<2:59:52, 64.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   8%|▊         | 14/180 [47:54<2:34:39, 55.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   8%|▊         | 15/180 [48:12<2:02:17, 44.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   9%|▉         | 16/180 [48:15<1:27:51, 32.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:   9%|▉         | 17/180 [1:26:49<32:31:04, 718.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  10%|█         | 18/180 [1:29:11<24:32:00, 545.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  11%|█         | 19/180 [1:29:13<17:04:45, 381.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  11%|█         | 20/180 [1:29:16<11:55:13, 268.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  12%|█▏        | 21/180 [1:29:18<8:18:38, 188.17s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  12%|█▏        | 22/180 [1:29:33<5:59:11, 136.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  13%|█▎        | 23/180 [1:29:35<4:11:00, 95.92s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  13%|█▎        | 24/180 [1:29:52<3:07:48, 72.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  14%|█▍        | 25/180 [1:30:33<2:42:18, 62.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  14%|█▍        | 26/180 [1:30:37<1:56:26, 45.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  15%|█▌        | 27/180 [1:31:08<1:44:05, 40.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  16%|█▌        | 28/180 [1:32:17<2:05:29, 49.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  16%|█▌        | 29/180 [1:32:22<1:30:24, 35.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  17%|█▋        | 30/180 [1:33:05<1:35:30, 38.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  17%|█▋        | 31/180 [1:34:06<1:51:59, 45.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  18%|█▊        | 32/180 [1:35:01<1:58:02, 47.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  18%|█▊        | 33/180 [1:35:48<1:56:37, 47.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  19%|█▉        | 34/180 [1:37:44<2:46:14, 68.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  19%|█▉        | 35/180 [1:38:29<2:28:18, 61.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  20%|██        | 36/180 [1:38:38<1:49:37, 45.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  21%|██        | 37/180 [1:38:47<1:22:14, 34.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  21%|██        | 38/180 [2:12:09<24:38:26, 624.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  22%|██▏       | 39/180 [2:13:02<17:45:27, 453.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  22%|██▏       | 40/180 [2:13:19<12:32:31, 322.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  23%|██▎       | 41/180 [2:13:30<8:50:19, 228.92s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  23%|██▎       | 42/180 [2:15:30<7:31:14, 196.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  24%|██▍       | 43/180 [2:15:35<5:16:49, 138.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  24%|██▍       | 44/180 [2:17:42<5:07:03, 135.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  25%|██▌       | 45/180 [2:18:04<3:48:08, 101.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  26%|██▌       | 46/180 [2:23:22<6:11:18, 166.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  26%|██▌       | 47/180 [2:23:26<4:21:01, 117.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  27%|██▋       | 48/180 [2:23:46<3:14:05, 88.22s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  27%|██▋       | 49/180 [2:24:23<2:39:17, 72.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  28%|██▊       | 50/180 [2:26:17<3:04:35, 85.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  28%|██▊       | 51/180 [2:26:54<2:32:19, 70.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  29%|██▉       | 52/180 [2:28:58<3:04:59, 86.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  29%|██▉       | 53/180 [2:29:16<2:20:13, 66.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  30%|███       | 54/180 [2:29:36<1:49:42, 52.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  31%|███       | 55/180 [2:29:41<1:19:08, 37.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  31%|███       | 56/180 [2:56:49<17:44:32, 515.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  32%|███▏      | 57/180 [2:57:05<12:28:58, 365.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  32%|███▏      | 58/180 [2:57:14<8:45:36, 258.50s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  33%|███▎      | 59/180 [2:58:52<7:03:50, 210.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  33%|███▎      | 60/180 [2:59:01<4:59:43, 149.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  34%|███▍      | 61/180 [2:59:09<3:33:11, 107.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  34%|███▍      | 62/180 [2:59:58<2:56:53, 89.94s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  35%|███▌      | 63/180 [3:00:18<2:13:58, 68.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  36%|███▌      | 64/180 [3:00:27<1:38:20, 50.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  36%|███▌      | 65/180 [3:01:40<1:50:27, 57.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  37%|███▋      | 66/180 [3:02:13<1:35:25, 50.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  37%|███▋      | 67/180 [3:02:18<1:08:46, 36.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  38%|███▊      | 68/180 [3:02:27<53:00, 28.40s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  38%|███▊      | 69/180 [3:03:11<1:01:22, 33.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  39%|███▉      | 70/180 [3:04:24<1:22:37, 45.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  39%|███▉      | 71/180 [3:04:42<1:07:08, 36.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  40%|████      | 72/180 [3:05:01<56:27, 31.37s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  41%|████      | 73/180 [3:05:19<48:59, 27.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  41%|████      | 74/180 [3:07:19<1:37:19, 55.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  42%|████▏     | 75/180 [3:07:38<1:17:34, 44.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  42%|████▏     | 76/180 [3:07:43<56:15, 32.46s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  43%|████▎     | 77/180 [3:09:43<1:41:09, 58.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  43%|████▎     | 78/180 [3:10:24<1:30:50, 53.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  44%|████▍     | 79/180 [3:10:43<1:12:27, 43.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  44%|████▍     | 80/180 [3:10:52<54:47, 32.87s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  45%|████▌     | 81/180 [3:13:52<2:07:00, 76.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  46%|████▌     | 82/180 [3:39:47<14:10:03, 520.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  46%|████▌     | 83/180 [3:40:28<10:09:07, 376.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  47%|████▋     | 84/180 [3:42:22<7:56:37, 297.89s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  47%|████▋     | 85/180 [3:46:08<7:17:22, 276.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  48%|████▊     | 86/180 [3:46:23<5:09:48, 197.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  48%|████▊     | 87/180 [3:46:27<3:36:48, 139.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  49%|████▉     | 88/180 [3:48:29<3:25:58, 134.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  49%|████▉     | 89/180 [3:48:55<2:34:28, 101.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  50%|█████     | 90/180 [3:49:06<1:51:55, 74.62s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  51%|█████     | 91/180 [3:51:13<2:13:58, 90.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  51%|█████     | 92/180 [3:52:02<1:54:27, 78.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  52%|█████▏    | 93/180 [3:52:21<1:27:34, 60.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  52%|█████▏    | 94/180 [3:54:29<1:55:25, 80.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  53%|█████▎    | 95/180 [3:54:48<1:27:58, 62.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  53%|█████▎    | 96/180 [3:55:07<1:08:37, 49.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  54%|█████▍    | 97/180 [3:55:15<50:59, 36.86s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  54%|█████▍    | 98/180 [3:55:34<43:11, 31.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  55%|█████▌    | 99/180 [3:56:30<52:33, 38.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  56%|█████▌    | 100/180 [3:57:32<1:01:01, 45.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  56%|█████▌    | 101/180 [3:57:41<45:34, 34.61s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  57%|█████▋    | 102/180 [3:57:58<38:01, 29.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  57%|█████▋    | 103/180 [3:58:22<35:42, 27.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  58%|█████▊    | 104/180 [3:58:30<27:53, 22.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  58%|█████▊    | 105/180 [3:59:56<51:19, 41.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  59%|█████▉    | 106/180 [4:00:46<53:53, 43.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  59%|█████▉    | 107/180 [4:02:43<1:19:49, 65.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  60%|██████    | 108/180 [4:02:52<58:20, 48.62s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  61%|██████    | 109/180 [4:02:56<41:53, 35.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  61%|██████    | 110/180 [4:03:06<32:20, 27.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  62%|██████▏   | 111/180 [4:04:47<57:05, 49.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  62%|██████▏   | 112/180 [4:06:44<1:19:25, 70.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  63%|██████▎   | 113/180 [4:07:03<1:00:55, 54.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  63%|██████▎   | 114/180 [4:07:47<56:36, 51.46s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  64%|██████▍   | 115/180 [4:09:52<1:19:39, 73.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  64%|██████▍   | 116/180 [4:25:30<5:55:02, 332.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  65%|██████▌   | 117/180 [4:27:41<4:45:57, 272.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  66%|██████▌   | 118/180 [4:27:53<3:20:33, 194.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  66%|██████▌   | 119/180 [4:29:20<2:44:37, 161.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  67%|██████▋   | 120/180 [4:30:09<2:08:20, 128.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  67%|██████▋   | 121/180 [4:30:29<1:34:06, 95.70s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  68%|██████▊   | 122/180 [4:30:34<1:06:10, 68.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  68%|██████▊   | 123/180 [4:32:14<1:14:07, 78.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  69%|██████▉   | 124/180 [4:33:02<1:04:12, 68.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  69%|██████▉   | 125/180 [4:40:40<2:50:09, 185.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  70%|███████   | 126/180 [4:41:34<2:11:39, 146.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  71%|███████   | 127/180 [4:41:39<1:31:39, 103.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  71%|███████   | 128/180 [4:41:57<1:07:40, 78.09s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  72%|███████▏  | 129/180 [4:42:02<47:40, 56.08s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  72%|███████▏  | 130/180 [4:42:38<41:53, 50.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  73%|███████▎  | 131/180 [4:42:43<29:57, 36.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  73%|███████▎  | 132/180 [4:42:50<22:08, 27.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  74%|███████▍  | 133/180 [4:42:59<17:16, 22.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  74%|███████▍  | 134/180 [4:43:02<12:28, 16.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  75%|███████▌  | 135/180 [4:43:42<17:37, 23.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  76%|███████▌  | 136/180 [4:43:47<13:03, 17.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  76%|███████▌  | 137/180 [4:44:37<19:46, 27.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  77%|███████▋  | 138/180 [4:53:35<2:06:24, 180.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  77%|███████▋  | 139/180 [4:54:17<1:34:59, 139.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  78%|███████▊  | 140/180 [4:54:33<1:08:10, 102.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  78%|███████▊  | 141/180 [4:55:00<51:40, 79.51s/it]   The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  79%|███████▉  | 142/180 [4:57:02<58:34, 92.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  79%|███████▉  | 143/180 [4:58:34<56:57, 92.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  80%|████████  | 144/180 [4:58:44<40:26, 67.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  81%|████████  | 145/180 [4:58:48<28:21, 48.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  81%|████████  | 146/180 [5:16:44<3:22:12, 356.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  82%|████████▏ | 147/180 [5:18:47<2:37:40, 286.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  82%|████████▏ | 148/180 [5:19:40<1:55:31, 216.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  83%|████████▎ | 149/180 [5:19:50<1:19:46, 154.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  83%|████████▎ | 150/180 [5:26:09<1:50:59, 221.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  84%|████████▍ | 151/180 [5:28:10<1:32:33, 191.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  84%|████████▍ | 152/180 [5:28:21<1:04:08, 137.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  85%|████████▌ | 153/180 [5:28:49<47:04, 104.61s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  86%|████████▌ | 154/180 [5:30:25<44:09, 101.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  86%|████████▌ | 155/180 [5:30:35<31:03, 74.54s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  87%|████████▋ | 156/180 [5:32:34<35:09, 87.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  87%|████████▋ | 157/180 [5:34:34<37:20, 97.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  88%|████████▊ | 158/180 [5:34:56<27:23, 74.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  88%|████████▊ | 159/180 [5:35:00<18:46, 53.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  89%|████████▉ | 160/180 [5:35:38<16:15, 48.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  89%|████████▉ | 161/180 [5:37:39<22:19, 70.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  90%|█████████ | 162/180 [5:42:50<42:51, 142.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  91%|█████████ | 163/180 [5:43:09<29:52, 105.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  91%|█████████ | 164/180 [5:43:13<20:02, 75.17s/it] The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  92%|█████████▏| 165/180 [5:43:22<13:50, 55.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  92%|█████████▏| 166/180 [5:44:57<15:40, 67.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  93%|█████████▎| 167/180 [5:45:22<11:46, 54.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  93%|█████████▎| 168/180 [5:45:40<08:43, 43.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  94%|█████████▍| 169/180 [5:47:47<12:34, 68.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  94%|█████████▍| 170/180 [5:47:58<08:34, 51.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  95%|█████████▌| 171/180 [5:48:16<06:11, 41.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  96%|█████████▌| 172/180 [5:48:21<04:02, 30.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  96%|█████████▌| 173/180 [5:48:56<03:42, 31.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  97%|█████████▋| 174/180 [5:49:45<03:41, 36.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  97%|█████████▋| 175/180 [5:50:33<03:20, 40.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  98%|█████████▊| 176/180 [5:51:26<02:57, 44.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
olmocr Progress:  98%|█████████▊| 177/180 [5:51:34<01:39, 33.21s/it]olmocr Progress:  99%|█████████▉| 178/180 [6:00:10<05:56, 178.05s/it]olmocr Progress:  99%|█████████▉| 179/180 [6:04:35<03:24, 204.25s/it]olmocr Progress: 100%|██████████| 180/180 [6:08:16<00:00, 209.20s/it]olmocr Progress: 100%|██████████| 180/180 [6:08:16<00:00, 122.76s/it]
Successfully loaded allenai/olmOCR-7B-0225-preview
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
Successfully loaded allenai/olmOCR-7B-0225-preview
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
Successfully loaded allenai/olmOCR-7B-0225-preview
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
Successfully loaded allenai/olmOCR-7B-0225-preview
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
✓ Completed multiprocessing for olmocr

============================================================
Starting **multiprocessing** for model: rolmocr
Total items to process: 180
Using 4 worker processes
============================================================
rolmocr Progress:   0%|          | 0/180 [00:00<?, ?it/s][rolmocr] Initializing VLMProcessor
Loading reducto/RolmOCR
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][rolmocr] Initializing VLMProcessor
Loading reducto/RolmOCR
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][rolmocr] Initializing VLMProcessor
Loading reducto/RolmOCR
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][rolmocr] Initializing VLMProcessor
Loading reducto/RolmOCR
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   1%|          | 1/180 [00:36<1:49:18, 36.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   1%|          | 2/180 [00:38<48:12, 16.25s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   2%|▏         | 3/180 [00:40<28:22,  9.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   2%|▏         | 4/180 [00:42<19:04,  6.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   3%|▎         | 5/180 [02:23<1:59:08, 40.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   3%|▎         | 6/180 [02:41<1:35:35, 32.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   4%|▍         | 7/180 [02:42<1:05:05, 22.57s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   4%|▍         | 8/180 [03:15<1:14:15, 25.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   5%|▌         | 9/180 [04:41<2:07:18, 44.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   6%|▌         | 10/180 [06:01<2:37:45, 55.68s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   6%|▌         | 11/180 [07:20<2:56:57, 62.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   7%|▋         | 12/180 [08:56<3:23:49, 72.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   7%|▋         | 13/180 [11:56<4:52:54, 105.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   8%|▊         | 14/180 [13:07<4:22:43, 94.96s/it] The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   8%|▊         | 15/180 [14:16<3:59:37, 87.14s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   9%|▉         | 16/180 [30:33<16:10:06, 354.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:   9%|▉         | 17/180 [30:35<11:16:05, 248.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  10%|█         | 18/180 [30:37<7:51:28, 174.62s/it] The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  11%|█         | 19/180 [30:39<5:29:17, 122.72s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  11%|█         | 20/180 [30:41<3:50:27, 86.42s/it] The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  12%|█▏        | 21/180 [30:43<2:41:58, 61.12s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  12%|█▏        | 22/180 [30:45<1:54:02, 43.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  13%|█▎        | 23/180 [30:46<1:20:49, 30.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  13%|█▎        | 24/180 [30:48<57:48, 22.23s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  14%|█▍        | 25/180 [30:50<41:42, 16.14s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Successfully loaded reducto/RolmOCR
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config32.tiff: CUDA out of memory. Tried to allocate 122.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 17.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.68 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.83 GiB is allocated by PyTorch, and 360.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config41.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 19.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.68 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 452.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config6.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 19.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.68 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 452.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config25.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 19.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.68 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 452.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config16.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 19.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.68 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 452.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config8.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config19.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config27.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config36.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config31.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  14%|█▍        | 26/180 [30:52<30:31, 11.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  15%|█▌        | 27/180 [30:54<22:36,  8.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  16%|█▌        | 28/180 [30:56<17:16,  6.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  16%|█▌        | 29/180 [30:58<13:27,  5.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  17%|█▋        | 30/180 [31:00<10:42,  4.28s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  17%|█▋        | 31/180 [31:02<08:58,  3.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  18%|█▊        | 32/180 [31:04<07:34,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  18%|█▊        | 33/180 [31:06<06:51,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  19%|█▉        | 34/180 [31:08<06:10,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  19%|█▉        | 35/180 [31:10<05:42,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config43.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config6.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config9.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config15.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config40.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config1.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config14.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config19.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config24.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config9.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  20%|██        | 36/180 [31:12<05:15,  2.19s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  21%|██        | 37/180 [31:14<05:07,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  21%|██        | 38/180 [31:15<04:50,  2.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  22%|██▏       | 39/180 [31:17<04:46,  2.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  22%|██▏       | 40/180 [31:20<04:45,  2.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  23%|██▎       | 41/180 [31:21<04:33,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  23%|██▎       | 42/180 [31:23<04:35,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  24%|██▍       | 43/180 [31:25<04:26,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  24%|██▍       | 44/180 [31:27<04:30,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  25%|██▌       | 45/180 [31:29<04:27,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config27.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config33.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config44.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config13.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config4.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config31.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config4.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config25.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config28.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config5.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  26%|██▌       | 46/180 [31:31<04:19,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  26%|██▌       | 47/180 [31:34<04:34,  2.07s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  27%|██▋       | 48/180 [31:35<04:28,  2.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  27%|██▋       | 49/180 [31:37<04:17,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  28%|██▊       | 50/180 [31:39<04:16,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  28%|██▊       | 51/180 [31:41<04:17,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  29%|██▉       | 52/180 [31:43<04:08,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  29%|██▉       | 53/180 [31:45<04:06,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  30%|███       | 54/180 [31:47<04:04,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  31%|███       | 55/180 [31:49<04:03,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config42.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config7.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config38.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config9.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config3.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config10.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config3.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config33.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config2.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config16.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  31%|███       | 56/180 [31:51<03:57,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  32%|███▏      | 57/180 [31:53<04:01,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  32%|███▏      | 58/180 [31:55<03:59,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  33%|███▎      | 59/180 [31:57<03:56,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  33%|███▎      | 60/180 [31:59<03:48,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  34%|███▍      | 61/180 [32:01<03:59,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  34%|███▍      | 62/180 [32:03<03:55,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  35%|███▌      | 63/180 [32:05<03:52,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  36%|███▌      | 64/180 [32:07<03:44,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  36%|███▌      | 65/180 [32:09<03:44,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config39.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config20.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config39.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config37.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config35.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config38.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config24.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config28.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config32.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config1.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  37%|███▋      | 66/180 [32:11<03:46,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  37%|███▋      | 67/180 [32:13<03:44,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  38%|███▊      | 68/180 [32:15<03:43,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  38%|███▊      | 69/180 [32:16<03:34,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  39%|███▉      | 70/180 [32:18<03:33,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  39%|███▉      | 71/180 [32:20<03:36,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  40%|████      | 72/180 [32:22<03:34,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  41%|████      | 73/180 [32:24<03:31,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  41%|████      | 74/180 [32:27<03:35,  2.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  42%|████▏     | 75/180 [32:29<03:30,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config18.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config7.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config1.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config10.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config44.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config2.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config30.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config35.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config22.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config12.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  42%|████▏     | 76/180 [32:30<03:27,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  43%|████▎     | 77/180 [32:32<03:24,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  43%|████▎     | 78/180 [32:34<03:22,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  44%|████▍     | 79/180 [32:36<03:19,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  44%|████▍     | 80/180 [32:38<03:11,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  45%|████▌     | 81/180 [32:40<03:14,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  46%|████▌     | 82/180 [32:42<03:11,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  46%|████▌     | 83/180 [32:44<03:09,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  47%|████▋     | 84/180 [32:46<03:07,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  47%|████▋     | 85/180 [32:48<03:05,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config25.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config19.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config27.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config4.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config33.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config29.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config36.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config10.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config2.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config28.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  48%|████▊     | 86/180 [32:50<03:00,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  48%|████▊     | 87/180 [32:52<03:08,  2.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  49%|████▉     | 88/180 [32:54<03:04,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  49%|████▉     | 89/180 [32:56<02:57,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  50%|█████     | 90/180 [32:58<02:58,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  51%|█████     | 91/180 [33:00<02:52,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  51%|█████     | 92/180 [33:02<02:50,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  52%|█████▏    | 93/180 [33:04<02:49,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  52%|█████▏    | 94/180 [33:06<02:43,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  53%|█████▎    | 95/180 [33:08<02:46,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config0.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config43.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config10.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config23.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config11.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config24.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config21.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config40.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config15.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config2.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  53%|█████▎    | 96/180 [33:10<02:45,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  54%|█████▍    | 97/180 [33:12<02:42,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  54%|█████▍    | 98/180 [33:14<02:41,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  55%|█████▌    | 99/180 [33:15<02:35,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  56%|█████▌    | 100/180 [33:18<02:40,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  56%|█████▌    | 101/180 [33:20<02:39,  2.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  57%|█████▋    | 102/180 [33:21<02:32,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  57%|█████▋    | 103/180 [33:23<02:31,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  58%|█████▊    | 104/180 [33:25<02:31,  2.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  58%|█████▊    | 105/180 [33:27<02:29,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config3.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config30.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config29.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config18.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config5.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config39.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config38.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config13.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config30.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config38.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  59%|█████▉    | 106/180 [33:29<02:26,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  59%|█████▉    | 107/180 [33:31<02:24,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  60%|██████    | 108/180 [33:33<02:18,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  61%|██████    | 109/180 [33:35<02:20,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  61%|██████    | 110/180 [33:37<02:14,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  62%|██████▏   | 111/180 [33:39<02:13,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  62%|██████▏   | 112/180 [33:41<02:14,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  63%|██████▎   | 113/180 [33:43<02:11,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  63%|██████▎   | 114/180 [33:45<02:14,  2.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  64%|██████▍   | 115/180 [33:47<02:10,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config22.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config26.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config40.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config7.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config5.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config11.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config11.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config41.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config23.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config9.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  64%|██████▍   | 116/180 [33:49<02:04,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  65%|██████▌   | 117/180 [33:51<02:02,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  66%|██████▌   | 118/180 [33:53<02:03,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  66%|██████▌   | 119/180 [33:55<02:00,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  67%|██████▋   | 120/180 [33:57<01:58,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  67%|██████▋   | 121/180 [33:59<01:56,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  68%|██████▊   | 122/180 [34:01<01:52,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  68%|██████▊   | 123/180 [34:03<01:50,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  69%|██████▉   | 124/180 [34:05<01:48,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  69%|██████▉   | 125/180 [34:07<01:44,  1.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config12.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config17.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config1.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config21.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config20.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config39.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config34.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config6.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config17.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config31.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  70%|███████   | 126/180 [34:08<01:43,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  71%|███████   | 127/180 [34:10<01:42,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  71%|███████   | 128/180 [34:13<01:44,  2.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  72%|███████▏  | 129/180 [34:14<01:40,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  72%|███████▏  | 130/180 [34:16<01:38,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  73%|███████▎  | 131/180 [34:19<01:37,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  73%|███████▎  | 132/180 [34:20<01:35,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  74%|███████▍  | 133/180 [34:22<01:30,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  74%|███████▍  | 134/180 [34:24<01:30,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  75%|███████▌  | 135/180 [34:26<01:28,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config22.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config34.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config37.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config34.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config6.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config43.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config44.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config44.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config37.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config35.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  76%|███████▌  | 136/180 [34:28<01:26,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  76%|███████▌  | 137/180 [34:30<01:24,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  77%|███████▋  | 138/180 [34:32<01:22,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  77%|███████▋  | 139/180 [34:34<01:20,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  78%|███████▊  | 140/180 [34:36<01:18,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  78%|███████▊  | 141/180 [34:38<01:14,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  79%|███████▉  | 142/180 [34:40<01:18,  2.06s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  79%|███████▉  | 143/180 [34:42<01:15,  2.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  80%|████████  | 144/180 [34:44<01:10,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  81%|████████  | 145/180 [34:44<00:51,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  81%|████████  | 146/180 [34:46<00:47,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config34.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config18.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config40.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config32.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config42.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config28.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config26.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config8.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config41.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config43.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 173.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  82%|████████▏ | 147/180 [34:47<00:49,  1.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  82%|████████▏ | 148/180 [34:49<00:48,  1.53s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  83%|████████▎ | 149/180 [34:50<00:47,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  83%|████████▎ | 150/180 [34:52<00:46,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  84%|████████▍ | 151/180 [34:54<00:45,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  84%|████████▍ | 152/180 [34:55<00:43,  1.57s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  85%|████████▌ | 153/180 [34:57<00:42,  1.58s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  86%|████████▌ | 154/180 [34:58<00:41,  1.58s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  86%|████████▌ | 155/180 [35:00<00:39,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  87%|████████▋ | 156/180 [35:02<00:37,  1.57s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config20.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config26.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config42.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 117.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config21.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config8.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 117.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config11.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 173.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config12.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config14.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config3.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 117.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config18.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 173.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  87%|████████▋ | 157/180 [35:04<00:39,  1.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  88%|████████▊ | 158/180 [35:05<00:35,  1.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  88%|████████▊ | 159/180 [35:07<00:34,  1.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  89%|████████▉ | 160/180 [35:08<00:33,  1.66s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  89%|████████▉ | 161/180 [35:10<00:30,  1.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  90%|█████████ | 162/180 [35:12<00:29,  1.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  91%|█████████ | 163/180 [35:13<00:27,  1.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  91%|█████████ | 164/180 [35:15<00:26,  1.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  92%|█████████▏| 165/180 [35:17<00:25,  1.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  92%|█████████▏| 166/180 [35:19<00:24,  1.77s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config17.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config14.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 117.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config25.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 173.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config17.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config5.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 117.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config23.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config32.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config16.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 229.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.87 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config36.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config4.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  93%|█████████▎| 167/180 [35:21<00:23,  1.84s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  93%|█████████▎| 168/180 [35:23<00:22,  1.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  94%|█████████▍| 169/180 [35:24<00:20,  1.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  94%|█████████▍| 170/180 [35:27<00:19,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  95%|█████████▌| 171/180 [35:28<00:16,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  96%|█████████▌| 172/180 [35:30<00:15,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  96%|█████████▌| 173/180 [35:32<00:13,  1.87s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  97%|█████████▋| 174/180 [35:34<00:11,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  97%|█████████▋| 175/180 [35:36<00:09,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
rolmocr Progress:  98%|█████████▊| 176/180 [35:38<00:07,  1.86s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config37.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config36.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config13.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config8.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config12.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config16.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config21.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config26.tiff: CUDA out of memory. Tried to allocate 496.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.34 GiB is allocated by PyTorch, and 653.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2019-07-12#01_config15.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 227.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 692.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2014-06-02#01_config24.tiff: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 115.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 355.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
rolmocr Progress:  98%|█████████▊| 177/180 [35:40<00:05,  1.87s/it]rolmocr Progress:  99%|█████████▉| 178/180 [36:56<00:48, 24.24s/it]rolmocr Progress:  99%|█████████▉| 179/180 [1:03:33<08:16, 496.12s/it]rolmocr Progress: 100%|██████████| 180/180 [1:26:53<00:00, 767.11s/it]rolmocr Progress: 100%|██████████| 180/180 [1:26:53<00:00, 28.96s/it] 
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config33.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Successfully loaded reducto/RolmOCR
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config42.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 449.62 MiB is free. Process 1145291 has 20.07 GiB memory in use. Including non-PyTorch memory, this process has 18.44 GiB memory in use. Process 1145292 has 20.07 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 659.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config29.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 361.62 MiB is free. Process 1145291 has 20.07 GiB memory in use. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Process 1145292 has 20.07 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 747.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2017-05-15#04_config41.tiff: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 417.62 MiB is free. Process 1145291 has 20.07 GiB memory in use. Including non-PyTorch memory, this process has 18.47 GiB memory in use. Process 1145292 has 20.07 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.30 GiB is allocated by PyTorch, and 691.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config30.tiff: CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 117.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Including non-PyTorch memory, this process has 18.87 GiB memory in use. Process 1145292 has 18.58 GiB memory in use. Process 1145294 has 20.20 GiB memory in use. Of the allocated memory 17.99 GiB is allocated by PyTorch, and 394.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
Successfully loaded reducto/RolmOCR
VLM detected, logging to OCR log.
Error processing image ./results/images/preprocessed/gestion#2015-09-10#04_config27.tiff: CUDA out of memory. Tried to allocate 248.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.62 MiB is free. Process 1145291 has 21.44 GiB memory in use. Process 1145293 has 18.88 GiB memory in use. Process 1145292 has 18.53 GiB memory in use. Including non-PyTorch memory, this process has 20.20 GiB memory in use. Of the allocated memory 18.93 GiB is allocated by PyTorch, and 789.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
VLM detected, logging to OCR log.
✓ Completed multiprocessing for rolmocr

============================================================
Starting **multiprocessing** for model: nanonets
Total items to process: 180
Using 4 worker processes
============================================================
nanonets Progress:   0%|          | 0/180 [00:00<?, ?it/s][nanonets] Initializing VLMProcessor
Loading nanonets/Nanonets-OCR-s
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][nanonets] Initializing VLMProcessor
Loading nanonets/Nanonets-OCR-s
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][nanonets] Initializing VLMProcessor
Loading nanonets/Nanonets-OCR-s
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][nanonets] Initializing VLMProcessor
Loading nanonets/Nanonets-OCR-s
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   1%|          | 1/180 [00:31<1:33:15, 31.26s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   1%|          | 2/180 [01:29<2:19:17, 46.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   2%|▏         | 3/180 [01:35<1:23:11, 28.20s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   2%|▏         | 4/180 [01:47<1:04:15, 21.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   3%|▎         | 5/180 [03:50<2:50:02, 58.30s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   3%|▎         | 6/180 [04:49<2:49:54, 58.59s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   4%|▍         | 7/180 [04:55<1:59:17, 41.37s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   4%|▍         | 8/180 [05:07<1:32:07, 32.14s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   5%|▌         | 9/180 [05:21<1:15:33, 26.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   6%|▌         | 10/180 [05:38<1:06:24, 23.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   6%|▌         | 11/180 [05:52<58:19, 20.70s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   7%|▋         | 12/180 [05:57<44:30, 15.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   7%|▋         | 13/180 [06:40<1:06:37, 23.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   8%|▊         | 14/180 [07:10<1:11:14, 25.75s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   8%|▊         | 15/180 [07:16<55:10, 20.06s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   9%|▉         | 16/180 [07:29<48:23, 17.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:   9%|▉         | 17/180 [07:29<34:16, 12.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  10%|█         | 18/180 [08:19<1:03:50, 23.65s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  11%|█         | 19/180 [08:37<59:20, 22.11s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  11%|█         | 20/180 [08:55<55:13, 20.71s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  12%|█▏        | 21/180 [08:57<39:56, 15.07s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  12%|█▏        | 22/180 [09:16<42:39, 16.20s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  13%|█▎        | 23/180 [09:35<45:16, 17.30s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  13%|█▎        | 24/180 [09:36<31:49, 12.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  14%|█▍        | 25/180 [10:48<1:17:47, 30.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  14%|█▍        | 26/180 [11:06<1:08:20, 26.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  15%|█▌        | 27/180 [11:45<1:17:34, 30.42s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  16%|█▌        | 28/180 [12:14<1:15:59, 30.00s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  16%|█▌        | 29/180 [12:56<1:23:53, 33.33s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  17%|█▋        | 30/180 [12:56<58:48, 23.52s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  17%|█▋        | 31/180 [13:27<1:03:53, 25.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  18%|█▊        | 32/180 [13:50<1:01:35, 24.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  18%|█▊        | 33/180 [14:18<1:03:27, 25.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  19%|█▉        | 34/180 [14:38<58:38, 24.10s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  19%|█▉        | 35/180 [15:19<1:10:28, 29.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  20%|██        | 36/180 [15:41<1:05:02, 27.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  21%|██        | 37/180 [15:53<53:26, 22.42s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  21%|██        | 38/180 [16:15<52:26, 22.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  22%|██▏       | 39/180 [16:46<58:51, 25.05s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  22%|██▏       | 40/180 [16:56<47:20, 20.29s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  23%|██▎       | 41/180 [17:28<55:10, 23.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  23%|██▎       | 42/180 [17:29<39:23, 17.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  24%|██▍       | 43/180 [17:48<40:03, 17.55s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  24%|██▍       | 44/180 [19:01<1:17:59, 34.41s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  25%|██▌       | 45/180 [19:03<55:15, 24.56s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  26%|██▌       | 46/180 [19:13<45:03, 20.17s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  26%|██▌       | 47/180 [19:31<43:23, 19.57s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  27%|██▋       | 48/180 [20:06<53:19, 24.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  27%|██▋       | 49/180 [20:43<1:01:19, 28.08s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  28%|██▊       | 50/180 [21:07<58:20, 26.93s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  28%|██▊       | 51/180 [21:46<1:05:05, 30.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  29%|██▉       | 52/180 [22:13<1:02:39, 29.37s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  29%|██▉       | 53/180 [22:21<48:49, 23.07s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  30%|███       | 54/180 [22:23<34:55, 16.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  31%|███       | 55/180 [22:41<35:30, 17.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  31%|███       | 56/180 [23:08<41:16, 19.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  32%|███▏      | 57/180 [23:18<34:48, 16.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  32%|███▏      | 58/180 [23:38<36:48, 18.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  33%|███▎      | 59/180 [23:47<30:57, 15.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  33%|███▎      | 60/180 [24:21<41:35, 20.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  34%|███▍      | 61/180 [25:32<1:11:13, 35.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  34%|███▍      | 62/180 [25:44<56:19, 28.64s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  35%|███▌      | 63/180 [26:00<48:43, 24.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  36%|███▌      | 64/180 [26:02<35:06, 18.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  36%|███▌      | 65/180 [26:58<56:24, 29.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  37%|███▋      | 66/180 [27:36<1:01:02, 32.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  37%|███▋      | 67/180 [27:45<47:18, 25.12s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  38%|███▊      | 68/180 [27:55<38:14, 20.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  38%|███▊      | 69/180 [28:33<47:52, 25.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  39%|███▉      | 70/180 [28:52<43:16, 23.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  39%|███▉      | 71/180 [29:20<45:22, 24.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  40%|████      | 72/180 [29:28<36:00, 20.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  41%|████      | 73/180 [29:33<27:31, 15.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  41%|████      | 74/180 [29:48<27:06, 15.35s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  42%|████▏     | 75/180 [30:18<34:36, 19.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  42%|████▏     | 76/180 [30:37<33:48, 19.51s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  43%|████▎     | 77/180 [31:05<37:56, 22.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  43%|████▎     | 78/180 [31:53<50:26, 29.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  44%|████▍     | 79/180 [32:01<38:58, 23.15s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  44%|████▍     | 80/180 [32:39<46:02, 27.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  45%|████▌     | 81/180 [33:13<49:03, 29.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  46%|████▌     | 82/180 [34:27<1:10:11, 42.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  46%|████▌     | 83/180 [35:13<1:10:39, 43.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  47%|████▋     | 84/180 [35:21<53:00, 33.13s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  47%|████▋     | 85/180 [36:33<1:11:06, 44.91s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  48%|████▊     | 86/180 [36:46<55:15, 35.27s/it]  The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  48%|████▊     | 87/180 [37:05<46:51, 30.23s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  49%|████▉     | 88/180 [37:15<37:14, 24.29s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  49%|████▉     | 89/180 [37:41<37:29, 24.72s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  50%|█████     | 90/180 [37:49<29:32, 19.69s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  51%|█████     | 91/180 [38:17<33:06, 22.32s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  51%|█████     | 92/180 [38:19<23:49, 16.25s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  52%|█████▏    | 93/180 [38:41<25:59, 17.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  52%|█████▏    | 94/180 [38:58<25:25, 17.74s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  53%|█████▎    | 95/180 [39:06<20:47, 14.68s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  53%|█████▎    | 96/180 [39:23<21:27, 15.33s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  54%|█████▍    | 97/180 [39:55<28:03, 20.29s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  54%|█████▍    | 98/180 [40:20<29:49, 21.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  55%|█████▌    | 99/180 [40:21<20:47, 15.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  56%|█████▌    | 100/180 [41:51<50:43, 38.04s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  56%|█████▌    | 101/180 [41:53<35:30, 26.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  57%|█████▋    | 102/180 [42:04<28:49, 22.17s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  57%|█████▋    | 103/180 [42:42<34:40, 27.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  58%|█████▊    | 104/180 [43:00<30:44, 24.27s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  58%|█████▊    | 105/180 [43:14<26:40, 21.34s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  59%|█████▉    | 106/180 [43:19<20:13, 16.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  59%|█████▉    | 107/180 [43:58<28:05, 23.09s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  60%|██████    | 108/180 [44:41<35:05, 29.25s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  61%|██████    | 109/180 [45:00<30:49, 26.05s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  61%|██████    | 110/180 [45:12<25:31, 21.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  62%|██████▏   | 111/180 [45:34<25:16, 21.98s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  62%|██████▏   | 112/180 [46:19<32:41, 28.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  63%|██████▎   | 113/180 [46:26<24:54, 22.31s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  63%|██████▎   | 114/180 [46:48<24:20, 22.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  64%|██████▍   | 115/180 [46:58<20:04, 18.53s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  64%|██████▍   | 116/180 [47:18<20:03, 18.81s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  65%|██████▌   | 117/180 [47:26<16:24, 15.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  66%|██████▌   | 118/180 [47:45<17:13, 16.67s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  66%|██████▌   | 119/180 [48:03<17:20, 17.06s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  67%|██████▋   | 120/180 [48:11<14:25, 14.42s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  67%|██████▋   | 121/180 [48:25<13:52, 14.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  68%|██████▊   | 122/180 [48:32<11:43, 12.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  68%|██████▊   | 123/180 [48:43<11:15, 11.85s/it]nanonets Progress:  69%|██████▉   | 124/180 [48:43<07:47,  8.34s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  69%|██████▉   | 125/180 [48:46<06:10,  6.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  70%|███████   | 126/180 [49:24<14:25, 16.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  71%|███████   | 127/180 [49:42<14:45, 16.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  71%|███████   | 128/180 [49:46<11:01, 12.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  72%|███████▏  | 129/180 [50:01<11:22, 13.38s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  72%|███████▏  | 130/180 [50:04<08:41, 10.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  73%|███████▎  | 131/180 [50:08<06:58,  8.55s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  73%|███████▎  | 132/180 [50:23<08:11, 10.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  74%|███████▍  | 133/180 [50:40<09:42, 12.39s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  74%|███████▍  | 134/180 [50:45<07:44, 10.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  75%|███████▌  | 135/180 [53:02<36:14, 48.33s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  76%|███████▌  | 136/180 [53:21<28:53, 39.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  76%|███████▌  | 137/180 [53:43<24:28, 34.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  77%|███████▋  | 138/180 [54:01<20:28, 29.24s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  77%|███████▋  | 139/180 [54:05<14:57, 21.89s/it]nanonets Progress:  78%|███████▊  | 140/180 [54:06<10:16, 15.42s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  78%|███████▊  | 141/180 [54:36<12:54, 19.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  79%|███████▉  | 142/180 [56:09<26:26, 41.74s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  79%|███████▉  | 143/180 [57:03<28:01, 45.45s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  80%|████████  | 144/180 [57:21<22:17, 37.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  81%|████████  | 145/180 [57:22<15:23, 26.37s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  81%|████████  | 146/180 [57:25<11:04, 19.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  82%|████████▏ | 147/180 [57:39<09:44, 17.73s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  82%|████████▏ | 148/180 [57:51<08:38, 16.21s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  83%|████████▎ | 149/180 [58:06<08:08, 15.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  83%|████████▎ | 150/180 [58:22<07:56, 15.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  84%|████████▍ | 151/180 [58:48<09:02, 18.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  84%|████████▍ | 152/180 [58:55<07:10, 15.36s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  85%|████████▌ | 153/180 [58:56<04:57, 11.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  86%|████████▌ | 154/180 [59:25<07:03, 16.30s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  86%|████████▌ | 155/180 [59:50<07:53, 18.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  87%|████████▋ | 156/180 [59:55<05:57, 14.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  87%|████████▋ | 157/180 [1:00:03<04:51, 12.68s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  88%|████████▊ | 158/180 [1:00:05<03:27,  9.42s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  88%|████████▊ | 159/180 [1:00:23<04:12, 12.03s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  89%|████████▉ | 160/180 [1:00:33<03:52, 11.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  89%|████████▉ | 161/180 [1:00:59<05:02, 15.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  90%|█████████ | 162/180 [1:01:13<04:31, 15.11s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  91%|█████████ | 163/180 [1:01:36<05:00, 17.70s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  91%|█████████ | 164/180 [1:01:42<03:43, 13.97s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  92%|█████████▏| 165/180 [1:01:44<02:36, 10.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  92%|█████████▏| 166/180 [1:01:55<02:27, 10.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  93%|█████████▎| 167/180 [1:01:59<01:54,  8.84s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  93%|█████████▎| 168/180 [1:02:26<02:48, 14.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  94%|█████████▍| 169/180 [1:02:31<02:06, 11.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  94%|█████████▍| 170/180 [1:03:15<03:30, 21.07s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  95%|█████████▌| 171/180 [1:03:15<02:15, 15.01s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  96%|█████████▌| 172/180 [1:03:25<01:47, 13.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  96%|█████████▌| 173/180 [1:03:34<01:24, 12.06s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  97%|█████████▋| 174/180 [1:03:50<01:19, 13.32s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  97%|█████████▋| 175/180 [1:04:07<01:11, 14.22s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  98%|█████████▊| 176/180 [1:04:21<00:57, 14.41s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
nanonets Progress:  98%|█████████▊| 177/180 [1:04:49<00:55, 18.35s/it]nanonets Progress:  99%|█████████▉| 178/180 [1:04:58<00:31, 15.67s/it]nanonets Progress:  99%|█████████▉| 179/180 [1:06:39<00:41, 41.23s/it]nanonets Progress: 100%|██████████| 180/180 [1:07:11<00:00, 38.28s/it]nanonets Progress: 100%|██████████| 180/180 [1:07:11<00:00, 22.40s/it]
Successfully loaded nanonets/Nanonets-OCR-s
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
Successfully loaded nanonets/Nanonets-OCR-s
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
Successfully loaded nanonets/Nanonets-OCR-s
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, logging to OCR log.
VLM detected, loggin✓ Completed multiprocessing for nanonets
================================================================================
                                 FINAL SUMMARY                                  
================================================================================
Total execution time: 31342.5 seconds
Models processed: 3
Total OCR results per model: 5
Total processing tasks: 15
================================================================================
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Loaded 540 OCR results
Starting multithreaded LLM postprocessing...
Models to process: ['phi4']

============================================================
Starting **multiprocessing** for model: phi4
Total items to process: 540
Using 4 worker processes
============================================================
phi4 Progress:   0%|          | 0/540 [00:00<?, ?it/s]phi4 Progress:   0%|          | 1/540 [00:09<1:21:41,  9.09s/it]phi4 Progress:   0%|          | 2/540 [00:34<2:46:23, 18.56s/it]phi4 Progress:   1%|          | 3/540 [00:44<2:13:25, 14.91s/it]phi4 Progress:   1%|          | 4/540 [01:00<2:15:29, 15.17s/it]phi4 Progress:   1%|          | 5/540 [01:27<2:53:33, 19.46s/it]phi4 Progress:   1%|          | 6/540 [01:30<2:03:10, 13.84s/it]phi4 Progress:   1%|▏         | 7/540 [02:03<2:58:42, 20.12s/it]phi4 Progress:   1%|▏         | 8/540 [02:38<3:39:19, 24.74s/it]phi4 Progress:   2%|▏         | 9/540 [02:40<2:38:18, 17.89s/it]phi4 Progress:   2%|▏         | 10/540 [02:51<2:18:50, 15.72s/it]phi4 Progress:   2%|▏         | 11/540 [02:55<1:46:17, 12.06s/it]phi4 Progress:   2%|▏         | 12/540 [03:03<1:34:59, 10.79s/it]phi4 Progress:   2%|▏         | 13/540 [03:04<1:08:10,  7.76s/it]phi4 Progress:   3%|▎         | 14/540 [03:16<1:21:19,  9.28s/it]phi4 Progress:   3%|▎         | 15/540 [03:19<1:03:31,  7.26s/it]phi4 Progress:   3%|▎         | 16/540 [03:47<1:57:23, 13.44s/it]phi4 Progress:   3%|▎         | 17/540 [04:27<3:07:40, 21.53s/it]phi4 Progress:   3%|▎         | 18/540 [04:35<2:31:23, 17.40s/it]phi4 Progress:   4%|▎         | 19/540 [04:36<1:47:39, 12.40s/it]phi4 Progress:   4%|▎         | 20/540 [04:36<1:17:00,  8.88s/it]phi4 Progress:   4%|▍         | 21/540 [04:47<1:19:58,  9.25s/it]phi4 Progress:   4%|▍         | 22/540 [05:07<1:49:12, 12.65s/it]phi4 Progress:   4%|▍         | 23/540 [05:23<1:58:26, 13.75s/it]phi4 Progress:   4%|▍         | 24/540 [05:26<1:29:04, 10.36s/it]phi4 Progress:   5%|▍         | 25/540 [05:26<1:03:19,  7.38s/it]phi4 Progress:   5%|▍         | 26/540 [05:33<1:00:40,  7.08s/it]phi4 Progress:   5%|▌         | 27/540 [05:47<1:19:05,  9.25s/it]phi4 Progress:   5%|▌         | 28/540 [06:27<2:37:13, 18.43s/it]phi4 Progress:   5%|▌         | 29/540 [06:47<2:40:09, 18.80s/it]phi4 Progress:   6%|▌         | 30/540 [07:12<2:57:49, 20.92s/it]phi4 Progress:   6%|▌         | 31/540 [07:20<2:22:25, 16.79s/it]phi4 Progress:   6%|▌         | 32/540 [07:33<2:13:36, 15.78s/it]phi4 Progress:   6%|▌         | 33/540 [07:46<2:06:50, 15.01s/it]phi4 Progress:   6%|▋         | 34/540 [07:58<1:57:47, 13.97s/it]phi4 Progress:   6%|▋         | 35/540 [07:59<1:25:57, 10.21s/it]phi4 Progress:   7%|▋         | 36/540 [08:02<1:06:02,  7.86s/it]phi4 Progress:   7%|▋         | 37/540 [08:24<1:42:35, 12.24s/it]phi4 Progress:   7%|▋         | 38/540 [08:34<1:37:20, 11.64s/it]phi4 Progress:   7%|▋         | 39/540 [08:36<1:12:39,  8.70s/it]phi4 Progress:   7%|▋         | 40/540 [08:37<53:48,  6.46s/it]  phi4 Progress:   8%|▊         | 41/540 [08:50<1:08:32,  8.24s/it]phi4 Progress:   8%|▊         | 42/540 [08:51<50:55,  6.14s/it]  phi4 Progress:   8%|▊         | 43/540 [09:17<1:40:41, 12.16s/it]phi4 Progress:   8%|▊         | 44/540 [09:28<1:36:50, 11.71s/it]phi4 Progress:   8%|▊         | 45/540 [09:45<1:51:03, 13.46s/it]phi4 Progress:   9%|▊         | 46/540 [09:46<1:19:23,  9.64s/it]phi4 Progress:   9%|▊         | 47/540 [09:49<1:01:31,  7.49s/it]phi4 Progress:   9%|▉         | 48/540 [09:52<52:40,  6.42s/it]  phi4 Progress:   9%|▉         | 49/540 [10:03<1:03:42,  7.79s/it]phi4 Progress:   9%|▉         | 50/540 [10:21<1:26:26, 10.58s/it]phi4 Progress:   9%|▉         | 51/540 [10:22<1:03:33,  7.80s/it]phi4 Progress:  10%|▉         | 52/540 [10:29<1:02:43,  7.71s/it]phi4 Progress:  10%|▉         | 53/540 [10:40<1:10:50,  8.73s/it]phi4 Progress:  10%|█         | 54/540 [10:41<50:28,  6.23s/it]  phi4 Progress:  10%|█         | 55/540 [11:08<1:40:33, 12.44s/it]phi4 Progress:  10%|█         | 56/540 [11:15<1:28:26, 10.96s/it]phi4 Progress:  11%|█         | 57/540 [11:17<1:06:02,  8.20s/it]phi4 Progress:  11%|█         | 58/540 [11:21<56:46,  7.07s/it]  phi4 Progress:  11%|█         | 59/540 [11:32<1:04:48,  8.08s/it]phi4 Progress:  11%|█         | 60/540 [11:34<50:35,  6.32s/it]  phi4 Progress:  11%|█▏        | 61/540 [11:44<58:11,  7.29s/it]phi4 Progress:  11%|█▏        | 62/540 [11:47<47:49,  6.00s/it]phi4 Progress:  12%|█▏        | 63/540 [11:48<36:27,  4.58s/it]phi4 Progress:  12%|█▏        | 64/540 [12:08<1:13:59,  9.33s/it]phi4 Progress:  12%|█▏        | 65/540 [12:15<1:07:02,  8.47s/it]phi4 Progress:  12%|█▏        | 66/540 [12:15<47:49,  6.05s/it]  phi4 Progress:  12%|█▏        | 67/540 [12:23<51:08,  6.49s/it]phi4 Progress:  13%|█▎        | 68/540 [12:31<54:31,  6.93s/it]phi4 Progress:  13%|█▎        | 69/540 [12:48<1:19:36, 10.14s/it]phi4 Progress:  13%|█▎        | 70/540 [12:54<1:08:20,  8.72s/it]phi4 Progress:  13%|█▎        | 71/540 [13:06<1:15:56,  9.72s/it]phi4 Progress:  13%|█▎        | 72/540 [13:07<55:04,  7.06s/it]  phi4 Progress:  14%|█▎        | 73/540 [13:17<1:02:55,  8.08s/it]phi4 Progress:  14%|█▎        | 74/540 [13:28<1:10:15,  9.05s/it]phi4 Progress:  14%|█▍        | 75/540 [13:29<50:02,  6.46s/it]  phi4 Progress:  14%|█▍        | 76/540 [13:39<59:24,  7.68s/it]phi4 Progress:  14%|█▍        | 77/540 [13:51<1:08:13,  8.84s/it]phi4 Progress:  14%|█▍        | 78/540 [14:03<1:14:29,  9.67s/it]phi4 Progress:  15%|█▍        | 79/540 [14:04<54:45,  7.13s/it]  phi4 Progress:  15%|█▍        | 80/540 [14:05<41:05,  5.36s/it]phi4 Progress:  15%|█▌        | 81/540 [14:46<2:03:34, 16.15s/it]phi4 Progress:  15%|█▌        | 82/540 [15:03<2:05:25, 16.43s/it]phi4 Progress:  15%|█▌        | 83/540 [15:14<1:51:02, 14.58s/it]phi4 Progress:  16%|█▌        | 84/540 [15:44<2:25:57, 19.21s/it]phi4 Progress:  16%|█▌        | 85/540 [15:51<1:58:20, 15.61s/it]phi4 Progress:  16%|█▌        | 86/540 [15:52<1:24:24, 11.15s/it]phi4 Progress:  16%|█▌        | 87/540 [16:26<2:16:02, 18.02s/it]phi4 Progress:  16%|█▋        | 88/540 [16:43<2:13:44, 17.75s/it]phi4 Progress:  16%|█▋        | 89/540 [16:48<1:45:40, 14.06s/it]phi4 Progress:  17%|█▋        | 90/540 [16:49<1:16:29, 10.20s/it]phi4 Progress:  17%|█▋        | 91/540 [16:55<1:06:20,  8.87s/it]phi4 Progress:  17%|█▋        | 92/540 [17:06<1:11:03,  9.52s/it]phi4 Progress:  17%|█▋        | 93/540 [17:22<1:24:29, 11.34s/it]phi4 Progress:  17%|█▋        | 94/540 [17:58<2:19:29, 18.77s/it]phi4 Progress:  18%|█▊        | 95/540 [18:09<2:02:18, 16.49s/it]phi4 Progress:  18%|█▊        | 96/540 [18:12<1:32:06, 12.45s/it]phi4 Progress:  18%|█▊        | 97/540 [18:20<1:21:07, 10.99s/it]phi4 Progress:  18%|█▊        | 98/540 [18:28<1:14:59, 10.18s/it]phi4 Progress:  18%|█▊        | 99/540 [18:54<1:49:21, 14.88s/it]phi4 Progress:  19%|█▊        | 100/540 [18:55<1:19:33, 10.85s/it]phi4 Progress:  19%|█▊        | 101/540 [19:15<1:37:59, 13.39s/it]phi4 Progress:  19%|█▉        | 102/540 [19:24<1:28:47, 12.16s/it]phi4 Progress:  19%|█▉        | 103/540 [19:25<1:05:21,  8.97s/it]phi4 Progress:  19%|█▉        | 104/540 [19:54<1:47:05, 14.74s/it]phi4 Progress:  19%|█▉        | 105/540 [20:00<1:28:46, 12.25s/it]phi4 Progress:  20%|█▉        | 106/540 [20:03<1:07:39,  9.35s/it]phi4 Progress:  20%|█▉        | 107/540 [20:04<49:56,  6.92s/it]  phi4 Progress:  20%|██        | 108/540 [20:04<35:46,  4.97s/it]phi4 Progress:  20%|██        | 109/540 [20:06<28:30,  3.97s/it]phi4 Progress:  20%|██        | 110/540 [20:31<1:13:49, 10.30s/it]phi4 Progress:  21%|██        | 111/540 [20:35<59:23,  8.31s/it]  phi4 Progress:  21%|██        | 112/540 [20:41<55:22,  7.76s/it]phi4 Progress:  21%|██        | 113/540 [20:51<1:00:38,  8.52s/it]phi4 Progress:  21%|██        | 114/540 [21:08<1:17:38, 10.94s/it]phi4 Progress:  21%|██▏       | 115/540 [21:40<2:02:54, 17.35s/it]phi4 Progress:  21%|██▏       | 116/540 [21:52<1:49:32, 15.50s/it]phi4 Progress:  22%|██▏       | 117/540 [21:55<1:24:32, 11.99s/it]phi4 Progress:  22%|██▏       | 118/540 [22:17<1:45:47, 15.04s/it]phi4 Progress:  22%|██▏       | 119/540 [22:19<1:17:34, 11.06s/it]phi4 Progress:  22%|██▏       | 120/540 [22:25<1:06:51,  9.55s/it]phi4 Progress:  22%|██▏       | 121/540 [22:26<47:29,  6.80s/it]  phi4 Progress:  23%|██▎       | 122/540 [22:32<47:19,  6.79s/it]phi4 Progress:  23%|██▎       | 123/540 [22:38<45:20,  6.52s/it]phi4 Progress:  23%|██▎       | 124/540 [22:51<57:32,  8.30s/it]phi4 Progress:  23%|██▎       | 125/540 [23:14<1:29:05, 12.88s/it]phi4 Progress:  23%|██▎       | 126/540 [23:16<1:05:02,  9.43s/it]phi4 Progress:  24%|██▎       | 127/540 [23:32<1:19:53, 11.61s/it]phi4 Progress:  24%|██▎       | 128/540 [23:35<1:00:09,  8.76s/it]phi4 Progress:  24%|██▍       | 129/540 [23:47<1:08:20,  9.98s/it]phi4 Progress:  24%|██▍       | 130/540 [23:49<50:20,  7.37s/it]  phi4 Progress:  24%|██▍       | 131/540 [23:55<48:56,  7.18s/it]phi4 Progress:  24%|██▍       | 132/540 [23:57<36:45,  5.41s/it]phi4 Progress:  25%|██▍       | 133/540 [24:11<55:49,  8.23s/it]phi4 Progress:  25%|██▍       | 134/540 [24:12<40:23,  5.97s/it]phi4 Progress:  25%|██▌       | 135/540 [24:13<28:58,  4.29s/it]phi4 Progress:  25%|██▌       | 136/540 [24:19<34:12,  5.08s/it]phi4 Progress:  25%|██▌       | 137/540 [24:35<55:36,  8.28s/it]phi4 Progress:  26%|██▌       | 138/540 [24:50<1:08:07, 10.17s/it]phi4 Progress:  26%|██▌       | 139/540 [24:52<51:36,  7.72s/it]  phi4 Progress:  26%|██▌       | 140/540 [24:54<40:13,  6.03s/it]phi4 Progress:  26%|██▌       | 141/540 [24:59<38:13,  5.75s/it]phi4 Progress:  26%|██▋       | 142/540 [25:21<1:11:02, 10.71s/it]phi4 Progress:  26%|██▋       | 143/540 [25:23<52:27,  7.93s/it]  phi4 Progress:  27%|██▋       | 144/540 [25:23<37:34,  5.69s/it]phi4 Progress:  27%|██▋       | 145/540 [25:43<1:06:21, 10.08s/it]phi4 Progress:  27%|██▋       | 146/540 [25:53<1:04:53,  9.88s/it]phi4 Progress:  27%|██▋       | 147/540 [26:01<1:00:24,  9.22s/it]phi4 Progress:  27%|██▋       | 148/540 [26:02<44:40,  6.84s/it]  phi4 Progress:  28%|██▊       | 149/540 [26:22<1:09:41, 10.69s/it]phi4 Progress:  28%|██▊       | 150/540 [26:59<2:02:23, 18.83s/it]phi4 Progress:  28%|██▊       | 151/540 [27:14<1:53:44, 17.54s/it]phi4 Progress:  28%|██▊       | 152/540 [27:50<2:30:09, 23.22s/it]phi4 Progress:  28%|██▊       | 153/540 [28:14<2:31:27, 23.48s/it]phi4 Progress:  29%|██▊       | 154/540 [28:20<1:56:14, 18.07s/it]phi4 Progress:  29%|██▊       | 155/540 [28:33<1:46:58, 16.67s/it]phi4 Progress:  29%|██▉       | 156/540 [29:08<2:22:09, 22.21s/it]phi4 Progress:  29%|██▉       | 157/540 [29:15<1:52:29, 17.62s/it]phi4 Progress:  29%|██▉       | 158/540 [29:17<1:21:34, 12.81s/it]phi4 Progress:  29%|██▉       | 159/540 [29:24<1:10:08, 11.04s/it]phi4 Progress:  30%|██▉       | 160/540 [29:42<1:22:45, 13.07s/it]phi4 Progress:  30%|██▉       | 161/540 [30:11<1:52:36, 17.83s/it]phi4 Progress:  30%|███       | 162/540 [30:37<2:08:15, 20.36s/it]phi4 Progress:  30%|███       | 163/540 [30:38<1:30:58, 14.48s/it]phi4 Progress:  30%|███       | 164/540 [30:40<1:07:52, 10.83s/it]phi4 Progress:  31%|███       | 165/540 [30:47<1:00:22,  9.66s/it]phi4 Progress:  31%|███       | 166/540 [31:07<1:19:22, 12.73s/it]phi4 Progress:  31%|███       | 167/540 [31:25<1:29:46, 14.44s/it]phi4 Progress:  31%|███       | 168/540 [31:30<1:11:03, 11.46s/it]phi4 Progress:  31%|███▏      | 169/540 [31:33<55:25,  8.96s/it]  phi4 Progress:  31%|███▏      | 170/540 [31:43<57:21,  9.30s/it]phi4 Progress:  32%|███▏      | 171/540 [31:44<41:25,  6.74s/it]phi4 Progress:  32%|███▏      | 172/540 [32:03<1:04:32, 10.52s/it]phi4 Progress:  32%|███▏      | 173/540 [32:16<1:08:09, 11.14s/it]phi4 Progress:  32%|███▏      | 174/540 [32:23<1:01:47, 10.13s/it]phi4 Progress:  32%|███▏      | 175/540 [32:27<49:59,  8.22s/it]  phi4 Progress:  33%|███▎      | 176/540 [32:32<43:14,  7.13s/it]phi4 Progress:  33%|███▎      | 177/540 [33:03<1:26:23, 14.28s/it]phi4 Progress:  33%|███▎      | 178/540 [33:23<1:37:46, 16.21s/it]phi4 Progress:  33%|███▎      | 179/540 [33:53<2:01:07, 20.13s/it]phi4 Progress:  33%|███▎      | 180/540 [33:56<1:31:05, 15.18s/it]phi4 Progress:  34%|███▎      | 181/540 [33:57<1:05:26, 10.94s/it]phi4 Progress:  34%|███▎      | 182/540 [33:58<46:45,  7.84s/it]  phi4 Progress:  34%|███▍      | 183/540 [33:59<34:16,  5.76s/it]phi4 Progress:  34%|███▍      | 184/540 [34:01<27:04,  4.56s/it]phi4 Progress:  34%|███▍      | 185/540 [34:29<1:09:05, 11.68s/it]phi4 Progress:  34%|███▍      | 186/540 [34:37<1:02:28, 10.59s/it]phi4 Progress:  35%|███▍      | 187/540 [34:42<53:16,  9.06s/it]  phi4 Progress:  35%|███▍      | 188/540 [34:56<1:00:52, 10.38s/it]phi4 Progress:  35%|███▌      | 189/540 [35:11<1:08:08, 11.65s/it]phi4 Progress:  35%|███▌      | 190/540 [35:15<55:54,  9.58s/it]  phi4 Progress:  35%|███▌      | 191/540 [35:24<53:51,  9.26s/it]phi4 Progress:  36%|███▌      | 192/540 [35:32<51:56,  8.96s/it]phi4 Progress:  36%|███▌      | 193/540 [35:47<1:01:38, 10.66s/it]phi4 Progress:  36%|███▌      | 194/540 [35:56<59:40, 10.35s/it]  phi4 Progress:  36%|███▌      | 195/540 [35:57<43:27,  7.56s/it]phi4 Progress:  36%|███▋      | 196/540 [35:59<33:32,  5.85s/it]phi4 Progress:  36%|███▋      | 197/540 [36:00<24:45,  4.33s/it]phi4 Progress:  37%|███▋      | 198/540 [36:00<18:03,  3.17s/it]phi4 Progress:  37%|███▋      | 199/540 [36:01<13:40,  2.41s/it]phi4 Progress:  37%|███▋      | 200/540 [36:02<10:36,  1.87s/it]phi4 Progress:  37%|███▋      | 201/540 [36:02<08:19,  1.47s/it]phi4 Progress:  37%|███▋      | 202/540 [36:03<07:10,  1.27s/it]phi4 Progress:  38%|███▊      | 203/540 [36:04<05:53,  1.05s/it]phi4 Progress:  38%|███▊      | 204/540 [36:04<05:30,  1.02it/s]phi4 Progress:  38%|███▊      | 205/540 [36:05<04:22,  1.27it/s]phi4 Progress:  38%|███▊      | 206/540 [36:05<03:59,  1.39it/s]phi4 Progress:  38%|███▊      | 207/540 [36:06<03:21,  1.66it/s]phi4 Progress:  39%|███▊      | 208/540 [36:06<03:44,  1.48it/s]phi4 Progress:  39%|███▊      | 209/540 [36:07<03:12,  1.72it/s]phi4 Progress:  39%|███▉      | 210/540 [36:08<03:35,  1.53it/s]phi4 Progress:  39%|███▉      | 212/540 [36:09<03:16,  1.67it/s]phi4 Progress:  39%|███▉      | 213/540 [36:09<02:40,  2.04it/s]phi4 Progress:  40%|███▉      | 214/540 [36:10<03:23,  1.60it/s]phi4 Progress:  40%|███▉      | 215/540 [36:10<03:01,  1.80it/s]phi4 Progress:  40%|████      | 216/540 [36:11<03:22,  1.60it/s]phi4 Progress:  40%|████      | 218/540 [36:12<03:09,  1.70it/s]phi4 Progress:  41%|████      | 220/540 [36:13<03:10,  1.68it/s]phi4 Progress:  41%|████      | 221/540 [36:14<03:46,  1.41it/s]phi4 Progress:  41%|████      | 222/540 [36:15<03:52,  1.37it/s]phi4 Progress:  41%|████▏     | 223/540 [36:16<03:32,  1.49it/s]phi4 Progress:  41%|████▏     | 224/540 [36:16<03:03,  1.73it/s]phi4 Progress:  42%|████▏     | 225/540 [36:17<03:09,  1.67it/s]phi4 Progress:  42%|████▏     | 226/540 [36:17<02:58,  1.76it/s]phi4 Progress:  42%|████▏     | 227/540 [36:18<03:24,  1.53it/s]phi4 Progress:  42%|████▏     | 228/540 [36:18<02:54,  1.79it/s]phi4 Progress:  42%|████▏     | 229/540 [36:20<03:59,  1.30it/s]phi4 Progress:  43%|████▎     | 230/540 [36:20<03:33,  1.45it/s]phi4 Progress:  43%|████▎     | 231/540 [36:21<03:48,  1.36it/s]phi4 Progress:  43%|████▎     | 232/540 [36:22<03:26,  1.49it/s]phi4 Progress:  43%|████▎     | 233/540 [36:22<03:21,  1.53it/s]phi4 Progress:  43%|████▎     | 234/540 [36:23<04:17,  1.19it/s]phi4 Progress:  44%|████▎     | 235/540 [36:25<04:34,  1.11it/s]phi4 Progress:  44%|████▎     | 236/540 [36:25<03:56,  1.28it/s]phi4 Progress:  44%|████▍     | 237/540 [36:26<03:41,  1.37it/s]phi4 Progress:  44%|████▍     | 238/540 [36:26<03:15,  1.55it/s]phi4 Progress:  44%|████▍     | 239/540 [36:27<03:20,  1.50it/s]phi4 Progress:  44%|████▍     | 240/540 [36:27<02:53,  1.73it/s]phi4 Progress:  45%|████▍     | 241/540 [36:28<03:13,  1.54it/s]phi4 Progress:  45%|████▍     | 242/540 [36:28<02:47,  1.77it/s]phi4 Progress:  45%|████▌     | 243/540 [36:29<03:08,  1.57it/s]phi4 Progress:  45%|████▌     | 244/540 [36:30<03:01,  1.63it/s]phi4 Progress:  45%|████▌     | 245/540 [36:30<03:15,  1.51it/s]phi4 Progress:  46%|████▌     | 246/540 [36:31<02:49,  1.73it/s]phi4 Progress:  46%|████▌     | 247/540 [36:32<03:07,  1.56it/s]phi4 Progress:  46%|████▌     | 248/540 [36:32<02:45,  1.76it/s]phi4 Progress:  46%|████▌     | 249/540 [36:33<03:20,  1.45it/s]phi4 Progress:  46%|████▋     | 250/540 [36:33<02:35,  1.86it/s]phi4 Progress:  46%|████▋     | 251/540 [36:34<03:16,  1.47it/s]phi4 Progress:  47%|████▋     | 252/540 [36:35<03:49,  1.25it/s]phi4 Progress:  47%|████▋     | 253/540 [36:36<03:40,  1.30it/s]phi4 Progress:  47%|████▋     | 254/540 [36:37<03:51,  1.23it/s]phi4 Progress:  47%|████▋     | 255/540 [36:37<03:00,  1.58it/s]phi4 Progress:  47%|████▋     | 256/540 [36:38<03:18,  1.43it/s]phi4 Progress:  48%|████▊     | 257/540 [36:38<02:28,  1.91it/s]phi4 Progress:  48%|████▊     | 258/540 [36:39<02:53,  1.63it/s]phi4 Progress:  48%|████▊     | 259/540 [36:40<03:27,  1.36it/s]phi4 Progress:  48%|████▊     | 260/540 [36:41<03:56,  1.19it/s]phi4 Progress:  48%|████▊     | 261/540 [36:42<04:12,  1.10it/s]phi4 Progress:  49%|████▊     | 262/540 [36:43<03:51,  1.20it/s]phi4 Progress:  49%|████▊     | 263/540 [36:44<03:47,  1.22it/s]phi4 Progress:  49%|████▉     | 264/540 [36:44<03:43,  1.24it/s]phi4 Progress:  49%|████▉     | 265/540 [36:45<02:56,  1.56it/s]phi4 Progress:  49%|████▉     | 266/540 [36:46<03:33,  1.28it/s]phi4 Progress:  50%|████▉     | 268/540 [36:47<03:06,  1.46it/s]phi4 Progress:  50%|████▉     | 269/540 [36:47<03:00,  1.51it/s]phi4 Progress:  50%|█████     | 270/540 [36:48<02:45,  1.64it/s]phi4 Progress:  50%|█████     | 271/540 [36:49<02:44,  1.63it/s]phi4 Progress:  50%|█████     | 272/540 [36:49<02:37,  1.70it/s]phi4 Progress:  51%|█████     | 273/540 [36:50<02:40,  1.66it/s]phi4 Progress:  51%|█████     | 274/540 [36:50<02:29,  1.77it/s]phi4 Progress:  51%|█████     | 275/540 [36:51<02:40,  1.65it/s]phi4 Progress:  51%|█████     | 276/540 [36:51<02:43,  1.62it/s]phi4 Progress:  51%|█████▏    | 277/540 [36:52<02:30,  1.74it/s]phi4 Progress:  51%|█████▏    | 278/540 [36:53<02:33,  1.71it/s]phi4 Progress:  52%|█████▏    | 279/540 [36:54<03:09,  1.37it/s]phi4 Progress:  52%|█████▏    | 280/540 [36:55<03:37,  1.19it/s]phi4 Progress:  52%|█████▏    | 281/540 [36:55<02:42,  1.59it/s]phi4 Progress:  52%|█████▏    | 282/540 [36:56<03:10,  1.36it/s]phi4 Progress:  52%|█████▏    | 283/540 [36:57<03:31,  1.21it/s]phi4 Progress:  53%|█████▎    | 284/540 [36:57<02:39,  1.60it/s]phi4 Progress:  53%|█████▎    | 285/540 [36:58<03:10,  1.34it/s]phi4 Progress:  53%|█████▎    | 287/540 [36:59<02:29,  1.70it/s]phi4 Progress:  53%|█████▎    | 288/540 [36:59<02:12,  1.91it/s]phi4 Progress:  54%|█████▎    | 289/540 [37:00<02:26,  1.72it/s]phi4 Progress:  54%|█████▎    | 290/540 [37:00<02:07,  1.96it/s]phi4 Progress:  54%|█████▍    | 291/540 [37:01<02:16,  1.83it/s]phi4 Progress:  54%|█████▍    | 292/540 [37:02<02:27,  1.68it/s]phi4 Progress:  54%|█████▍    | 293/540 [37:03<03:02,  1.36it/s]phi4 Progress:  54%|█████▍    | 294/540 [37:04<03:11,  1.29it/s]phi4 Progress:  55%|█████▍    | 295/540 [37:04<02:25,  1.68it/s]phi4 Progress:  55%|█████▍    | 296/540 [37:05<02:54,  1.40it/s]phi4 Progress:  55%|█████▌    | 297/540 [37:05<02:13,  1.82it/s]phi4 Progress:  55%|█████▌    | 298/540 [37:06<02:25,  1.66it/s]phi4 Progress:  55%|█████▌    | 299/540 [37:06<02:12,  1.82it/s]phi4 Progress:  56%|█████▌    | 300/540 [37:07<02:12,  1.82it/s]phi4 Progress:  56%|█████▌    | 301/540 [37:07<01:53,  2.10it/s]phi4 Progress:  56%|█████▌    | 302/540 [37:08<02:16,  1.75it/s]phi4 Progress:  56%|█████▌    | 303/540 [37:08<02:01,  1.96it/s]phi4 Progress:  56%|█████▋    | 304/540 [37:09<02:04,  1.90it/s]phi4 Progress:  56%|█████▋    | 305/540 [37:09<02:06,  1.86it/s]phi4 Progress:  57%|█████▋    | 306/540 [37:10<02:11,  1.78it/s]phi4 Progress:  57%|█████▋    | 307/540 [37:11<02:26,  1.59it/s]phi4 Progress:  57%|█████▋    | 308/540 [37:11<02:07,  1.81it/s]phi4 Progress:  57%|█████▋    | 309/540 [37:12<02:42,  1.42it/s]phi4 Progress:  57%|█████▋    | 310/540 [37:12<02:09,  1.78it/s]phi4 Progress:  58%|█████▊    | 311/540 [37:13<02:45,  1.38it/s]phi4 Progress:  58%|█████▊    | 313/540 [37:15<02:29,  1.52it/s]phi4 Progress:  58%|█████▊    | 314/540 [37:15<02:09,  1.75it/s]phi4 Progress:  58%|█████▊    | 315/540 [37:16<02:26,  1.54it/s]phi4 Progress:  59%|█████▊    | 316/540 [37:16<01:56,  1.93it/s]phi4 Progress:  59%|█████▊    | 317/540 [37:17<02:14,  1.66it/s]phi4 Progress:  59%|█████▉    | 318/540 [37:17<01:43,  2.15it/s]phi4 Progress:  59%|█████▉    | 319/540 [37:18<02:05,  1.76it/s]phi4 Progress:  59%|█████▉    | 320/540 [37:18<01:46,  2.07it/s]phi4 Progress:  59%|█████▉    | 321/540 [37:19<02:10,  1.68it/s]phi4 Progress:  60%|█████▉    | 322/540 [37:20<02:20,  1.55it/s]phi4 Progress:  60%|██████    | 324/540 [37:21<02:06,  1.70it/s]phi4 Progress:  60%|██████    | 325/540 [37:21<01:43,  2.08it/s]phi4 Progress:  60%|██████    | 326/540 [37:22<02:11,  1.62it/s]phi4 Progress:  61%|██████    | 327/540 [37:23<02:30,  1.42it/s]phi4 Progress:  61%|██████    | 329/540 [37:24<02:14,  1.57it/s]phi4 Progress:  61%|██████    | 330/540 [37:24<02:14,  1.56it/s]phi4 Progress:  61%|██████▏   | 331/540 [37:25<02:17,  1.52it/s]phi4 Progress:  61%|██████▏   | 332/540 [37:26<02:05,  1.66it/s]phi4 Progress:  62%|██████▏   | 333/540 [37:26<02:11,  1.57it/s]phi4 Progress:  62%|██████▏   | 334/540 [37:27<01:45,  1.96it/s]phi4 Progress:  62%|██████▏   | 335/540 [37:28<02:15,  1.51it/s]phi4 Progress:  62%|██████▏   | 336/540 [37:28<02:01,  1.68it/s]phi4 Progress:  62%|██████▏   | 337/540 [37:29<02:11,  1.54it/s]phi4 Progress:  63%|██████▎   | 338/540 [37:29<02:02,  1.65it/s]phi4 Progress:  63%|██████▎   | 339/540 [37:30<02:31,  1.32it/s]phi4 Progress:  63%|██████▎   | 340/540 [37:31<01:58,  1.68it/s]phi4 Progress:  63%|██████▎   | 341/540 [37:31<02:14,  1.48it/s]phi4 Progress:  63%|██████▎   | 342/540 [37:32<02:19,  1.42it/s]phi4 Progress:  64%|██████▎   | 343/540 [37:33<02:12,  1.49it/s]phi4 Progress:  64%|██████▎   | 344/540 [37:33<02:06,  1.56it/s]phi4 Progress:  64%|██████▍   | 345/540 [37:34<02:11,  1.49it/s]phi4 Progress:  64%|██████▍   | 346/540 [37:35<01:54,  1.70it/s]phi4 Progress:  64%|██████▍   | 347/540 [37:35<01:50,  1.74it/s]phi4 Progress:  64%|██████▍   | 348/540 [37:36<02:19,  1.37it/s]phi4 Progress:  65%|██████▍   | 349/540 [37:37<02:34,  1.24it/s]phi4 Progress:  65%|██████▍   | 350/540 [37:37<01:57,  1.61it/s]phi4 Progress:  65%|██████▌   | 351/540 [37:38<02:18,  1.37it/s]phi4 Progress:  65%|██████▌   | 352/540 [37:38<01:42,  1.83it/s]phi4 Progress:  65%|██████▌   | 353/540 [37:39<01:54,  1.64it/s]phi4 Progress:  66%|██████▌   | 354/540 [37:39<01:31,  2.02it/s]phi4 Progress:  66%|██████▌   | 355/540 [37:40<01:55,  1.60it/s]phi4 Progress:  66%|██████▌   | 356/540 [37:41<01:53,  1.62it/s]phi4 Progress:  66%|██████▌   | 357/540 [37:42<02:35,  1.17it/s]phi4 Progress:  66%|██████▋   | 358/540 [37:43<02:44,  1.11it/s]phi4 Progress:  66%|██████▋   | 359/540 [38:00<16:45,  5.56s/it]phi4 Progress:  67%|██████▋   | 360/540 [38:04<15:02,  5.01s/it]phi4 Progress:  67%|██████▋   | 361/540 [38:41<44:14, 14.83s/it]phi4 Progress:  67%|██████▋   | 362/540 [38:50<38:50, 13.09s/it]phi4 Progress:  67%|██████▋   | 363/540 [39:14<47:54, 16.24s/it]phi4 Progress:  67%|██████▋   | 364/540 [39:21<39:44, 13.55s/it]phi4 Progress:  68%|██████▊   | 365/540 [39:26<32:12, 11.04s/it]phi4 Progress:  68%|██████▊   | 366/540 [39:33<28:07,  9.70s/it]phi4 Progress:  68%|██████▊   | 367/540 [39:40<25:31,  8.85s/it]phi4 Progress:  68%|██████▊   | 368/540 [39:47<23:46,  8.30s/it]phi4 Progress:  68%|██████▊   | 369/540 [39:48<17:32,  6.16s/it]phi4 Progress:  69%|██████▊   | 370/540 [39:56<19:19,  6.82s/it]phi4 Progress:  69%|██████▊   | 371/540 [39:57<14:08,  5.02s/it]phi4 Progress:  69%|██████▉   | 372/540 [40:06<17:37,  6.30s/it]phi4 Progress:  69%|██████▉   | 373/540 [40:16<20:23,  7.33s/it]phi4 Progress:  69%|██████▉   | 374/540 [40:18<16:03,  5.80s/it]phi4 Progress:  69%|██████▉   | 375/540 [40:29<19:53,  7.24s/it]phi4 Progress:  70%|██████▉   | 376/540 [40:44<26:12,  9.59s/it]phi4 Progress:  70%|██████▉   | 377/540 [40:45<19:17,  7.10s/it]phi4 Progress:  70%|███████   | 378/540 [40:47<14:46,  5.47s/it]phi4 Progress:  70%|███████   | 379/540 [40:56<17:06,  6.38s/it]phi4 Progress:  70%|███████   | 380/540 [41:01<16:21,  6.13s/it]phi4 Progress:  71%|███████   | 381/540 [41:04<13:39,  5.15s/it]phi4 Progress:  71%|███████   | 382/540 [41:25<25:48,  9.80s/it]phi4 Progress:  71%|███████   | 383/540 [41:29<21:26,  8.19s/it]phi4 Progress:  71%|███████   | 384/540 [41:36<20:36,  7.92s/it]phi4 Progress:  71%|███████▏  | 385/540 [41:39<16:12,  6.27s/it]phi4 Progress:  71%|███████▏  | 386/540 [41:40<12:23,  4.83s/it]phi4 Progress:  72%|███████▏  | 387/540 [41:49<15:18,  6.01s/it]phi4 Progress:  72%|███████▏  | 388/540 [41:52<13:10,  5.20s/it]phi4 Progress:  72%|███████▏  | 389/540 [42:00<15:20,  6.10s/it]phi4 Progress:  72%|███████▏  | 390/540 [42:08<15:58,  6.39s/it]phi4 Progress:  72%|███████▏  | 391/540 [42:35<31:34, 12.71s/it]phi4 Progress:  73%|███████▎  | 392/540 [42:47<30:46, 12.48s/it]phi4 Progress:  73%|███████▎  | 393/540 [43:02<32:11, 13.14s/it]phi4 Progress:  73%|███████▎  | 394/540 [43:14<31:17, 12.86s/it]phi4 Progress:  73%|███████▎  | 395/540 [43:16<22:59,  9.51s/it]phi4 Progress:  73%|███████▎  | 396/540 [43:23<21:06,  8.79s/it]phi4 Progress:  74%|███████▎  | 397/540 [43:32<21:12,  8.90s/it]phi4 Progress:  74%|███████▎  | 398/540 [43:46<24:46, 10.47s/it]phi4 Progress:  74%|███████▍  | 399/540 [43:54<22:45,  9.69s/it]phi4 Progress:  74%|███████▍  | 400/540 [43:58<18:40,  8.00s/it]phi4 Progress:  74%|███████▍  | 401/540 [44:04<17:12,  7.43s/it]phi4 Progress:  74%|███████▍  | 402/540 [44:06<13:05,  5.69s/it]phi4 Progress:  75%|███████▍  | 403/540 [44:11<13:06,  5.74s/it]phi4 Progress:  75%|███████▍  | 404/540 [44:23<16:56,  7.47s/it]phi4 Progress:  75%|███████▌  | 405/540 [44:32<17:56,  7.98s/it]phi4 Progress:  75%|███████▌  | 406/540 [44:34<13:57,  6.25s/it]phi4 Progress:  75%|███████▌  | 407/540 [44:49<19:20,  8.73s/it]phi4 Progress:  76%|███████▌  | 408/540 [44:57<18:52,  8.58s/it]phi4 Progress:  76%|███████▌  | 409/540 [45:05<18:06,  8.29s/it]phi4 Progress:  76%|███████▌  | 410/540 [45:10<15:53,  7.34s/it]phi4 Progress:  76%|███████▌  | 411/540 [45:16<14:43,  6.85s/it]phi4 Progress:  76%|███████▋  | 412/540 [45:26<16:47,  7.87s/it]phi4 Progress:  76%|███████▋  | 413/540 [45:29<13:31,  6.39s/it]phi4 Progress:  77%|███████▋  | 414/540 [45:31<10:48,  5.15s/it]phi4 Progress:  77%|███████▋  | 415/540 [45:34<09:30,  4.57s/it]phi4 Progress:  77%|███████▋  | 416/540 [45:43<12:12,  5.91s/it]phi4 Progress:  77%|███████▋  | 417/540 [45:49<12:02,  5.87s/it]phi4 Progress:  77%|███████▋  | 418/540 [45:52<10:25,  5.13s/it]phi4 Progress:  78%|███████▊  | 419/540 [45:59<11:13,  5.56s/it]phi4 Progress:  78%|███████▊  | 420/540 [46:14<17:01,  8.51s/it]phi4 Progress:  78%|███████▊  | 421/540 [46:23<17:04,  8.61s/it]phi4 Progress:  78%|███████▊  | 422/540 [46:33<17:37,  8.96s/it]phi4 Progress:  78%|███████▊  | 423/540 [46:45<19:31, 10.01s/it]phi4 Progress:  79%|███████▊  | 424/540 [46:51<16:47,  8.68s/it]phi4 Progress:  79%|███████▊  | 425/540 [46:59<16:03,  8.38s/it]phi4 Progress:  79%|███████▉  | 426/540 [47:09<16:47,  8.84s/it]phi4 Progress:  79%|███████▉  | 427/540 [47:11<12:47,  6.79s/it]phi4 Progress:  79%|███████▉  | 428/540 [47:25<16:46,  8.98s/it]phi4 Progress:  79%|███████▉  | 429/540 [47:35<17:11,  9.29s/it]phi4 Progress:  80%|███████▉  | 430/540 [47:38<13:44,  7.50s/it]phi4 Progress:  80%|███████▉  | 431/540 [47:45<13:13,  7.28s/it]phi4 Progress:  80%|████████  | 432/540 [47:56<15:22,  8.54s/it]phi4 Progress:  80%|████████  | 433/540 [48:16<20:58, 11.76s/it]phi4 Progress:  80%|████████  | 434/540 [48:24<18:51, 10.68s/it]phi4 Progress:  81%|████████  | 435/540 [48:25<13:34,  7.76s/it]phi4 Progress:  81%|████████  | 436/540 [48:27<10:37,  6.13s/it]phi4 Progress:  81%|████████  | 437/540 [48:36<12:01,  7.01s/it]phi4 Progress:  81%|████████  | 438/540 [49:09<24:57, 14.68s/it]phi4 Progress:  81%|████████▏ | 439/540 [49:14<20:14, 12.03s/it]phi4 Progress:  81%|████████▏ | 440/540 [49:16<14:57,  8.97s/it]phi4 Progress:  82%|████████▏ | 441/540 [49:26<14:58,  9.07s/it]phi4 Progress:  82%|████████▏ | 442/540 [49:31<13:12,  8.09s/it]phi4 Progress:  82%|████████▏ | 443/540 [49:39<12:57,  8.02s/it]phi4 Progress:  82%|████████▏ | 444/540 [49:52<14:55,  9.33s/it]phi4 Progress:  82%|████████▏ | 445/540 [50:00<14:30,  9.17s/it]phi4 Progress:  83%|████████▎ | 446/540 [50:03<11:11,  7.14s/it]phi4 Progress:  83%|████████▎ | 447/540 [50:18<14:49,  9.57s/it]phi4 Progress:  83%|████████▎ | 448/540 [50:31<16:00, 10.44s/it]phi4 Progress:  83%|████████▎ | 449/540 [50:48<19:04, 12.58s/it]phi4 Progress:  83%|████████▎ | 450/540 [50:50<14:11,  9.46s/it]phi4 Progress:  84%|████████▎ | 451/540 [50:59<13:30,  9.10s/it]phi4 Progress:  84%|████████▎ | 452/540 [51:06<12:34,  8.57s/it]phi4 Progress:  84%|████████▍ | 453/540 [51:13<11:56,  8.24s/it]phi4 Progress:  84%|████████▍ | 454/540 [51:25<13:07,  9.15s/it]phi4 Progress:  84%|████████▍ | 455/540 [51:27<09:56,  7.01s/it]phi4 Progress:  84%|████████▍ | 456/540 [51:38<11:26,  8.17s/it]phi4 Progress:  85%|████████▍ | 457/540 [52:03<18:28, 13.36s/it]phi4 Progress:  85%|████████▍ | 458/540 [52:14<17:28, 12.79s/it]phi4 Progress:  85%|████████▌ | 459/540 [52:45<24:16, 17.99s/it]phi4 Progress:  85%|████████▌ | 460/540 [52:56<21:23, 16.05s/it]phi4 Progress:  85%|████████▌ | 461/540 [53:01<16:42, 12.69s/it]phi4 Progress:  86%|████████▌ | 462/540 [53:05<12:56,  9.96s/it]phi4 Progress:  86%|████████▌ | 463/540 [53:10<11:14,  8.75s/it]phi4 Progress:  86%|████████▌ | 464/540 [53:15<09:17,  7.33s/it]phi4 Progress:  86%|████████▌ | 465/540 [53:36<14:23, 11.51s/it]phi4 Progress:  86%|████████▋ | 466/540 [53:48<14:33, 11.80s/it]phi4 Progress:  86%|████████▋ | 467/540 [53:53<11:56,  9.82s/it]phi4 Progress:  87%|████████▋ | 468/540 [53:55<08:54,  7.43s/it]phi4 Progress:  87%|████████▋ | 469/540 [54:11<11:51, 10.03s/it]phi4 Progress:  87%|████████▋ | 470/540 [54:24<12:30, 10.72s/it]phi4 Progress:  87%|████████▋ | 471/540 [54:28<10:10,  8.84s/it]phi4 Progress:  87%|████████▋ | 472/540 [54:33<08:45,  7.72s/it]phi4 Progress:  88%|████████▊ | 473/540 [54:41<08:43,  7.82s/it]phi4 Progress:  88%|████████▊ | 474/540 [54:49<08:26,  7.68s/it]phi4 Progress:  88%|████████▊ | 475/540 [54:54<07:41,  7.09s/it]phi4 Progress:  88%|████████▊ | 476/540 [55:02<07:47,  7.31s/it]phi4 Progress:  88%|████████▊ | 477/540 [55:24<12:08, 11.56s/it]phi4 Progress:  89%|████████▊ | 478/540 [55:46<15:20, 14.85s/it]phi4 Progress:  89%|████████▊ | 479/540 [55:58<14:03, 13.82s/it]phi4 Progress:  89%|████████▉ | 480/540 [55:59<09:57,  9.95s/it]phi4 Progress:  89%|████████▉ | 481/540 [56:05<08:39,  8.80s/it]phi4 Progress:  89%|████████▉ | 482/540 [56:07<06:38,  6.88s/it]phi4 Progress:  89%|████████▉ | 483/540 [56:16<07:15,  7.64s/it]phi4 Progress:  90%|████████▉ | 484/540 [56:31<09:00,  9.64s/it]phi4 Progress:  90%|████████▉ | 485/540 [56:36<07:30,  8.18s/it]phi4 Progress:  90%|█████████ | 486/540 [56:37<05:35,  6.22s/it]phi4 Progress:  90%|█████████ | 487/540 [56:42<05:04,  5.74s/it]phi4 Progress:  90%|█████████ | 488/540 [56:54<06:40,  7.69s/it]phi4 Progress:  91%|█████████ | 489/540 [57:02<06:33,  7.72s/it]phi4 Progress:  91%|█████████ | 490/540 [57:13<07:21,  8.84s/it]phi4 Progress:  91%|█████████ | 491/540 [57:16<05:35,  6.85s/it]phi4 Progress:  91%|█████████ | 492/540 [57:23<05:37,  7.03s/it]phi4 Progress:  91%|█████████▏| 493/540 [57:37<07:06,  9.07s/it]phi4 Progress:  91%|█████████▏| 494/540 [57:41<05:51,  7.64s/it]phi4 Progress:  92%|█████████▏| 495/540 [57:43<04:28,  5.97s/it]phi4 Progress:  92%|█████████▏| 496/540 [57:47<03:55,  5.35s/it]phi4 Progress:  92%|█████████▏| 497/540 [57:54<04:08,  5.78s/it]phi4 Progress:  92%|█████████▏| 498/540 [58:04<04:51,  6.94s/it]phi4 Progress:  92%|█████████▏| 499/540 [58:09<04:24,  6.46s/it]phi4 Progress:  93%|█████████▎| 500/540 [58:14<04:01,  6.04s/it]phi4 Progress:  93%|█████████▎| 501/540 [58:16<03:04,  4.74s/it]phi4 Progress:  93%|█████████▎| 502/540 [58:25<03:55,  6.21s/it]phi4 Progress:  93%|█████████▎| 503/540 [58:32<03:55,  6.36s/it]phi4 Progress:  93%|█████████▎| 504/540 [58:40<04:02,  6.73s/it]phi4 Progress:  94%|█████████▎| 505/540 [58:52<04:55,  8.46s/it]phi4 Progress:  94%|█████████▎| 506/540 [58:54<03:43,  6.57s/it]phi4 Progress:  94%|█████████▍| 507/540 [59:01<03:42,  6.75s/it]phi4 Progress:  94%|█████████▍| 508/540 [59:08<03:30,  6.58s/it]phi4 Progress:  94%|█████████▍| 509/540 [59:19<04:06,  7.96s/it]phi4 Progress:  94%|█████████▍| 510/540 [59:31<04:33,  9.12s/it]phi4 Progress:  95%|█████████▍| 511/540 [59:44<05:04, 10.51s/it]phi4 Progress:  95%|█████████▍| 512/540 [59:52<04:30,  9.65s/it]phi4 Progress:  95%|█████████▌| 513/540 [1:00:04<04:36, 10.24s/it]phi4 Progress:  95%|█████████▌| 514/540 [1:00:14<04:28, 10.33s/it]phi4 Progress:  95%|█████████▌| 515/540 [1:00:21<03:49,  9.17s/it]phi4 Progress:  96%|█████████▌| 516/540 [1:00:33<04:03, 10.13s/it]phi4 Progress:  96%|█████████▌| 517/540 [1:00:39<03:21,  8.76s/it]phi4 Progress:  96%|█████████▌| 518/540 [1:00:42<02:34,  7.03s/it]phi4 Progress:  96%|█████████▌| 519/540 [1:00:51<02:42,  7.75s/it]phi4 Progress:  96%|█████████▋| 520/540 [1:00:58<02:31,  7.59s/it]phi4 Progress:  96%|█████████▋| 521/540 [1:01:05<02:20,  7.38s/it]phi4 Progress:  97%|█████████▋| 522/540 [1:01:35<04:12, 14.00s/it]phi4 Progress:  97%|█████████▋| 523/540 [1:01:48<03:56, 13.91s/it]phi4 Progress:  97%|█████████▋| 524/540 [1:01:54<03:04, 11.53s/it]phi4 Progress:  97%|█████████▋| 525/540 [1:01:56<02:08,  8.56s/it]phi4 Progress:  97%|█████████▋| 526/540 [1:01:59<01:36,  6.86s/it]phi4 Progress:  98%|█████████▊| 527/540 [1:02:04<01:22,  6.37s/it]phi4 Progress:  98%|█████████▊| 528/540 [1:02:10<01:16,  6.37s/it]phi4 Progress:  98%|█████████▊| 529/540 [1:02:12<00:52,  4.82s/it]phi4 Progress:  98%|█████████▊| 530/540 [1:02:20<00:58,  5.88s/it]phi4 Progress:  98%|█████████▊| 531/540 [1:02:25<00:51,  5.76s/it]phi4 Progress:  99%|█████████▊| 532/540 [1:02:27<00:36,  4.62s/it]phi4 Progress:  99%|█████████▊| 533/540 [1:02:35<00:38,  5.51s/it]phi4 Progress:  99%|█████████▉| 534/540 [1:02:38<00:29,  4.85s/it]phi4 Progress:  99%|█████████▉| 535/540 [1:02:48<00:31,  6.32s/it]phi4 Progress:  99%|█████████▉| 536/540 [1:03:02<00:34,  8.51s/it]phi4 Progress:  99%|█████████▉| 537/540 [1:03:16<00:30, 10.30s/it]phi4 Progress: 100%|█████████▉| 538/540 [1:03:21<00:17,  8.63s/it]phi4 Progress: 100%|█████████▉| 539/540 [1:03:31<00:09,  9.12s/it]phi4 Progress: 100%|██████████| 540/540 [1:03:31<00:00,  6.47s/it]phi4 Progress: 100%|██████████| 540/540 [1:03:31<00:00,  7.06s/it]
[phi4] Initializing Ollama model
[phi4] Initializing Ollama model
[phi4] Initializing Ollama model
[phi4] Initializing Ollama model
✓ Completed multiprocessing for phi4
================================================================================
                                 FINAL SUMMARY                                  
================================================================================
Total execution time: 3811.92 seconds
Models processed: 1
Total OCR results per model: 540
Total processing tasks: 540
================================================================================
