{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for message:\n",
      "\tmax: 3.000\n",
      "\tmin: 3.000\n",
      "\tmedian: 3.000\n",
      "\tmean: 3.000\n",
      "\tp95: 3.000\n",
      "\tp5: 3.000\n",
      "\n",
      "Statistics for token:\n",
      "\tmax: 318.000\n",
      "\tmin: 56.000\n",
      "\tmedian: 93.000\n",
      "\tmean: 100.954\n",
      "\tp95: 156.000\n",
      "\tp5: 65.000\n",
      "\n",
      "Statistics for ds_size:\n",
      "\t7000\n",
      "\n",
      "Automatically selected context length:  512\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "DATA_PATH = \"train_data/PMA_fine_tuning_FULL_SPIDER.jsonl\"\n",
    "SUPPORTED_CONTEXT_LENGTHS = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "# Import the tokenizer\n",
    "from transformers import LlamaTokenizerFast\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the dataset\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    items = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# Utility function for proper formatting of the data\n",
    "def convert_message_list_to_text(messages: list) -> str:\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    text = \"\"\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": messages[1][\"role\"],\n",
    "                \"content\": B_SYS\n",
    "                + messages[0][\"content\"]\n",
    "                + E_SYS\n",
    "                + messages[1][\"content\"],\n",
    "            }\n",
    "        ] + messages[2:]\n",
    "\n",
    "    assert all([msg[\"role\"] == \"user\" for msg in messages[::2]]) and all(\n",
    "            [msg[\"role\"] == \"assistant\" for msg in messages[1::2]]\n",
    "        ), (\n",
    "            \"model only supports 'system','user' and 'assistant' roles, \"\n",
    "            \"starting with user and alternating (u/a/u/a/u...)\"\n",
    "        )\n",
    "\n",
    "    texts = []\n",
    "    for prompt, answer in zip(messages[::2], messages[1::2]):\n",
    "        texts.append(f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \")\n",
    "\n",
    "    text = \"</s><s>\".join(texts)\n",
    "    # add the bos and eos token at the beginning of the first turn and the end of the last turn\n",
    "    text = \"<s>\" + text + \" </s>\"\n",
    "    # During training last message should be from assistant (not from a user)\n",
    "    assert (\n",
    "        messages[-1][\"role\"] == \"assistant\"\n",
    "    ), f\"Last message must be from assistant, got {messages[-1]['role']}\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Utility functions for calculating the statistics of the number of tokens in the dataset\n",
    "def print_token_statistics(stats) -> None:\n",
    "    for key in stats:\n",
    "        print(f\"Statistics for {key}:\")\n",
    "        if isinstance(stats[key], dict):\n",
    "            for stat_key, stat_value in stats[key].items():\n",
    "                print(f\"\\t{stat_key}: {stat_value:.3f}\")\n",
    "        else:\n",
    "            print(f\"\\t{stats[key]}\")\n",
    "        print(\"\")\n",
    "\n",
    "def get_tokenized_stats(items: list, print_stats: bool = True):\n",
    "\n",
    "    counters = defaultdict(list)\n",
    "    for batch in items:\n",
    "        messages = batch[\"messages\"]\n",
    "\n",
    "        # add message count\n",
    "        counters[\"message\"].append(len(messages))\n",
    "\n",
    "        # add the number of tokens of this message to the token counter\n",
    "        text = convert_message_list_to_text(messages)\n",
    "        tokens = tokenizer(text)['input_ids']\n",
    "        counters[\"token\"].append(len(tokens))\n",
    "\n",
    "    stats = {}\n",
    "    for key, value in counters.items():\n",
    "        stats[key] = {\n",
    "            \"max\": float(np.max(value)),\n",
    "            \"min\": float(np.min(value)),\n",
    "            \"median\": float(np.median(value)),\n",
    "            \"mean\": float(np.mean(value)),\n",
    "            \"p95\": float(np.percentile(value, 95)),\n",
    "            \"p5\": float(np.percentile(value, 5)),\n",
    "        }\n",
    "    stats[\"ds_size\"] = len(items)\n",
    "\n",
    "    if print_stats:\n",
    "        print_token_statistics(stats)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Auto calculate the context length\n",
    "stats = get_tokenized_stats(items, print_stats=True)\n",
    "for ctx_length in SUPPORTED_CONTEXT_LENGTHS:\n",
    "    if ctx_length > stats[\"token\"][\"p95\"]:\n",
    "        break\n",
    "\n",
    "print(\"Automatically selected context length: \", ctx_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"llama-8b\" # or 8x7b, 70b, ...\n",
    "DS_MAX_SIZE_LIMITS = {\n",
    "    \"mistral-7b\": {\n",
    "        512: 150_000,\n",
    "        1024: 50_000,\n",
    "        2048: 25_000,\n",
    "        4096: 10_000,\n",
    "        8192: 5_000,\n",
    "    },\n",
    "    \"llama-8b\": {\n",
    "        512: 90_000,\n",
    "        1024: 32_000,\n",
    "        2048: 15_000,\n",
    "        4096: 5_000,\n",
    "        8192: 5_000,\n",
    "        16384: 5_000,\n",
    "        32768: 2_500,\n",
    "    },\n",
    "    \"llama-70b\": {\n",
    "        512: 25_000,\n",
    "        1024: 10_000,\n",
    "        2048: 5_000,\n",
    "        4096: 5_000,\n",
    "        8192: 3_000,\n",
    "        16384: 1_500,\n",
    "    },\n",
    "    \"mixtral-8x7b\": {\n",
    "        512: 25_000,\n",
    "        1024: 10_000,\n",
    "        2048: 5_000,\n",
    "        4096: 5_000,\n",
    "        8192: 2_500,\n",
    "        16384: 1_000,\n",
    "        32768: 1_000,\n",
    "      },\n",
    "}\n",
    "CONTEXT_LENGTH = ctx_length\n",
    "\n",
    "ds_max_size = DS_MAX_SIZE_LIMITS[MODEL_SIZE][CONTEXT_LENGTH]\n",
    "if len(items) > ds_max_size:\n",
    "    raise ValueError(\n",
    "        f\"Dataset size ({len(items)}) exceeds the maximum allowable size ({ds_max_size})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 10:24:43,064\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-07-07_18-52-12_756286_35179/logs/ray-data\n",
      "2024-07-08 10:24:43,065\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(batched_convert_messages_to_text)]\n",
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in dataset per epoch:  706681\n",
      "Num tokens trained per epoch:  1169216\n",
      "Padding inflation ratio:  1.6545173847888934\n"
     ]
    }
   ],
   "source": [
    "# We will use ray data for batched data iteration\n",
    "import ray # pip install ray[data]\n",
    "import pandas as pd\n",
    "\n",
    "# You can change the batch size per device here\n",
    "BSIZE_PER_DEVICE = 16\n",
    "\n",
    "# Creating a ray dataset for easier processing\n",
    "df = pd.DataFrame.from_dict(items)\n",
    "ds = ray.data.from_pandas(df)\n",
    "\n",
    "\n",
    "def batched_convert_messages_to_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts a batch of messages (list of roles + content) to plain text.\"\"\"\n",
    "    df = []\n",
    "    for _, b in batch.iterrows():\n",
    "        text = convert_message_list_to_text(list(b[\"messages\"]))\n",
    "        df.append({\"input\": text})\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def collate_fn(batch: dict):\n",
    "    return tokenizer(\n",
    "        list(batch[\"input\"]),\n",
    "        padding=\"longest\",\n",
    "        max_length=CONTEXT_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Data preprocssing pipeline\n",
    "flattened_ds = ds.map_batches(\n",
    "    batched_convert_messages_to_text, batch_size=16, batch_format=\"pandas\"\n",
    ")\n",
    "\n",
    "data_set_tokens_per_epoch = 0\n",
    "trained_tokens_per_epoch = 0\n",
    "for batch in flattened_ds.iter_torch_batches(\n",
    "    batch_size=BSIZE_PER_DEVICE, collate_fn=collate_fn\n",
    "):\n",
    "    trained_tokens_per_epoch += batch[\"input_ids\"].numel()\n",
    "    data_set_tokens_per_epoch += batch[\"attention_mask\"].sum().item()\n",
    "\n",
    "print(\"Num tokens in dataset per epoch: \", data_set_tokens_per_epoch)\n",
    "print(\"Num tokens trained per epoch: \", trained_tokens_per_epoch)\n",
    "print(\"Padding inflation ratio: \", trained_tokens_per_epoch / data_set_tokens_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
