"""
evaluate.py

Evaluates the structured article outputs (CSV) generated by LLM postprocessing
against gold standard files. Calculates precision, recall, F1-score, and accuracy
for each config+OCR+LLM model combination.

Input:
- Gold standard CSVs: ./data/csv/*_goldstandard.csv
- Predicted CSVs: ./results/csv/extracted/*.csv

Output:
- Evaluation CSV: ./results/csv/evaluation.csv

Author: @nicolasleonri (GitHub)
License: GPL
"""

from utils_evaluate import *
from pathlib import Path
import unittest
import csv
import sys

def print_help() -> None:
    """Displays help instructions for the evaluation script."""
    print("""
    Evaluation Script - OCR Postprocessing Metrics
    ---------------------------------------------

    This script compares model-generated article CSVs against gold-standard CSVs and
    computes evaluation metrics (Accuracy, Precision, Recall, F1).

    Input:
        - Gold standards: ./data/csv/*_goldstandard.csv
        - Predictions:    ./results/csv/extracted/*.csv

    Output:
        - Evaluation results: ./results/csv/evaluation.csv

    Usage:
        python evaluate.py             Run evaluation
        python evaluate.py --test      Run test case
        python evaluate.py --help      Show this message
    """)

class TestEvaluation(unittest.TestCase):
    """Unit test for comparison logic using synthetic inputs."""

    def test_simple_similarity_evaluation(self):
        gold = [{
            "headline": "Test Headline",
            "subheadline": "A subtitle here",
            "content": "This is a short article."
        }]
        pred = [{
            "headline": "Test Headline",
            "subheadline": "A subtitle here",
            "content": "This is a short article."
        }]

        results = preprocess_data(gold, pred)
        y_true, y_pred = prepare_for_sklearn_metrics(results)
        acc, prec, rec, f1 = calculate_metrics(y_true, y_pred)

        self.assertEqual(y_true, [1, 1, 1])
        self.assertEqual(y_pred, [1, 1, 1])
        self.assertEqual((acc, prec, rec, f1), (1.0, 1.0, 1.0, 1.0))


def main() -> None:
    if "--help" in sys.argv or "-h" in sys.argv:
        print_help()
        return 
    
    if "--test" in sys.argv:
        unittest.main(argv=['first-arg-is-ignored'], exit=False)
        return

    final_results = []

    gold_standard_dir = "./data/csv/"
    results_dir = "./results/csv/extracted/"

    # Load all *_goldstandard.csv into a dict
    gold_standards = process_gold_standards(gold_standard_dir)

    results_path = Path(results_dir)
    eval_files = list(results_path.glob('*.csv'))

    best_f1 = 0.0

    for idx, eval_file in enumerate(eval_files, 1):
        total = len(eval_files)
        print(f"[{idx}/{total}] Processing: {eval_file.name}")

        # Extract ID, config, OCR engine, and LLM name from filename
        stem = eval_file.stem.split('_')[0] # e.g., newspaper#date#img3
        config = eval_file.stem.split('_')[1] # e.g., config42
        ocr_module = eval_file.stem.split('_')[2]
        llm_model = eval_file.stem.split('_')[3]

        if stem in gold_standards:
            gold_data = gold_standards[stem]
            try:
                eval_data = load_csv_file(str(eval_file))
                # eval_data = load_json_file(str(eval_file))
            except Exception as e:
                print(f"Error loading eval file {eval_file.name}: {e}")
                continue

            comparison_results = preprocess_data(gold_data, eval_data)
            
            if comparison_results == None:
                y_true, y_pred = [1, 1, 1], [0, 0, 0]
            else:
                y_true, y_pred = prepare_for_sklearn_metrics(comparison_results)

            (accuracy, precision, recall, f1) = calculate_metrics(y_true, y_pred)

            if f1 > best_f1:
                best_f1 = f1
                print(f"Best F1 so far: {best_f1:.5f}")
            
            if np.isnan(accuracy):
                accuracy = 0.0

            list_to_add = [stem, config[6:], ocr_module, llm_model, float(accuracy), float(precision), float(recall), float(f1)]
            final_results.append(list_to_add)

        else:
            print(f"Warning: No matching gold standard found for {eval_file.name}")
            return None
        
    # print(final_results)
    ocr_log_dict = parse_ocr_log('./results/txt/extracted/ocr_cropped_results_log.txt')
    print(ocr_log_dict)
    pre_log_dict = parse_preprocessing_log('./logs/preprocess.out')
    post_log_dict = parse_llm_logs_oneliner('./logs/postprocess.txt')

    def strip_extension(filename):
        return filename.replace('.tiff', '').replace('.png', '')

    for result in final_results:
        filename, config, ocr, llm = result[0], result[1], result[2], result[3]
        filename_stripped = strip_extension(filename)

        # Initialize extracted times
        pre_time_log = 0.0
        ocr_time_log = 0.0
        post_time_log = 0.0

        # Extract preprocessing time from logs
        for entry in pre_log_dict:
            if strip_extension(entry['filename']) == filename_stripped and str(entry['config']) == str(config):
                pre_time_log = entry['time_needed']
                break

        # Extract OCR time from logs
        for entry in ocr_log_dict:
            if (strip_extension(entry['filename']) == filename_stripped and
                str(entry['config']) == str(config) and
                entry['ocr'] == ocr):
                ocr_time_log = entry['time_needed']
                break

        # Extract postprocessing time from logs
        for entry in post_log_dict:
            if (strip_extension(entry['filename']) == filename_stripped and
                str(entry['config']) == str(config) and
                entry['llm'] == llm):
                
                # Special case for 'vlm' OCR
                if ocr == "vlm":
                    post_time_log = entry['time_needed']
                    break
                
                # Normal case: OCR must match exactly
                if entry['ocr'] == ocr:
                    post_time_log = entry['time_needed']
                    break

        # Replace None with "NA"
        pre_time_log = pre_time_log if pre_time_log is not None else "NA"
        ocr_time_log = ocr_time_log if ocr_time_log is not None else "NA"
        post_time_log = post_time_log if post_time_log is not None else "NA"

        # Calculate total time if all times are numbers, else "NA"
        if all(isinstance(t, (int, float)) for t in [pre_time_log, ocr_time_log, post_time_log]):
            total_time_log = pre_time_log + ocr_time_log + post_time_log
        else:
            total_time_log = "NA"

        # Append the extracted times as NEW columns to the row
        result.extend([pre_time_log, ocr_time_log, post_time_log, total_time_log])

    with open('./results/csv/evaluation.csv', 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        head = ['Filename', 'Config', 'OCR_module', 'LLM_model', "Accuracy", "Precision", "Recall", "F1_Score", "PP_Time", "OCR_Time", "LLM_Time", "Time"]
        csvwriter.writerow(head)

        # Write each row from the list
        for row in final_results:
            csvwriter.writerow(row)
    
    print(f"✅️ Evaluation done! File can be found under: './results/csv/evaluation.csv'")


if __name__ == "__main__":
    main()