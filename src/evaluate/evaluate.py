"""
evaluate.py

Evaluates the structured article outputs (CSV) generated by LLM postprocessing
against gold standard files. Calculates precision, recall, F1-score, and accuracy
for each config+OCR+LLM model combination.

Input:
- Gold standard CSVs: ./data/csv/*_goldstandard.csv
- Predicted CSVs: ./results/csv/extracted/*.csv

Output:
- Evaluation CSV: ./results/csv/evaluation.csv

Author: @nicolasleonri (GitHub)
License: GPL
"""

from utils_evaluate import *
from pathlib import Path
import unittest
import csv
import sys

def print_help() -> None:
    """Displays help instructions for the evaluation script."""
    print("""
    Evaluation Script - OCR Postprocessing Metrics
    ---------------------------------------------

    This script compares model-generated article CSVs against gold-standard CSVs and
    computes evaluation metrics (Accuracy, Precision, Recall, F1).

    Input:
        - Gold standards: ./data/csv/*_goldstandard.csv
        - Predictions:    ./results/csv/extracted/*.csv

    Output:
        - Evaluation results: ./results/csv/evaluation.csv

    Usage:
        python evaluate.py             Run evaluation
        python evaluate.py --test      Run test case
        python evaluate.py --help      Show this message
    """)

class TestEvaluation(unittest.TestCase):
    """Unit test for comparison logic using synthetic inputs."""

    def test_simple_similarity_evaluation(self):
        gold = [{
            "headline": "Test Headline",
            "subheadline": "A subtitle here",
            "content": "This is a short article."
        }]
        pred = [{
            "headline": "Test Headline",
            "subheadline": "A subtitle here",
            "content": "This is a short article."
        }]

        results = preprocess_data(gold, pred)
        y_true, y_pred = prepare_for_sklearn_metrics(results)
        acc, prec, rec, f1 = calculate_metrics(y_true, y_pred)

        self.assertEqual(y_true, [1, 1, 1])
        self.assertEqual(y_pred, [1, 1, 1])
        self.assertEqual((acc, prec, rec, f1), (1.0, 1.0, 1.0, 1.0))


def main() -> None:
    if "--help" in sys.argv or "-h" in sys.argv:
        print_help()
        return 
    
    if "--test" in sys.argv:
        unittest.main(argv=['first-arg-is-ignored'], exit=False)
        return

    final_results = []

    gold_standard_dir = "./data/csv/"
    results_dir = "./results/csv/extracted/"

    # Load all *_goldstandard.csv into a dict
    gold_standards = process_gold_standards(gold_standard_dir)

    results_path = Path(results_dir)
    eval_files = list(results_path.glob('*.csv'))

    best_f1 = 0.0

    for idx, eval_file in enumerate(eval_files, 1):
        total = len(eval_files)
        print(f"[{idx}/{total}] Processing: {eval_file.name}")

        # Extract ID, config, OCR engine, and LLM name from filename
        stem = eval_file.stem.split('_')[0] # e.g., newspaper#date#img3
        config = eval_file.stem.split('_')[1] # e.g., config42
        ocr_module = eval_file.stem.split('_')[2]
        llm_model = eval_file.stem.split('_')[3]

        if stem in gold_standards:
            gold_data = gold_standards[stem]
            try:
                eval_data = load_csv_file(str(eval_file))
                # eval_data = load_json_file(str(eval_file))
            except Exception as e:
                print(f"Error loading eval file {eval_file.name}: {e}")
                continue

            comparison_results = preprocess_data(gold_data, eval_data)
            
            if comparison_results == None:
                y_true, y_pred = [1, 1, 1], [0, 0, 0]
            else:
                y_true, y_pred = prepare_for_sklearn_metrics(comparison_results)

            (accuracy, precision, recall, f1) = calculate_metrics(y_true, y_pred)

            if f1 > best_f1:
                best_f1 = f1
                print(f"Best F1 so far: {best_f1:.5f}")
            
            if np.isnan(accuracy):
                accuracy = 0.0

            list_to_add = [stem, config[6:], ocr_module, llm_model, float(accuracy), float(precision), float(recall), float(f1)]
            final_results.append(list_to_add)

        else:
            print(f"Warning: No matching gold standard found for {eval_file.name}")
            return None

    with open('./results/csv/evaluation.csv', 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        head = ['Filename', 'Config', 'OCR_module', 'LLM_model', "Accuracy", "Precision", "Recall", "F1_Score"]
        csvwriter.writerow(head)

        # Write each row from the list
        for row in final_results:
            csvwriter.writerow(row)
    
    print(f"✅️ Evaluation done! File can be found under: './results/csv/evaluation.csv'")


if __name__ == "__main__":
    main()